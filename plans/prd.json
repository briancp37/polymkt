[
    {
        "category": "functional",
        "description": "Bootstrap cloud data lake: convert existing CSVs to partitioned Parquet and upload to S3 (source of truth)",
        "steps": [
            "Detect whether S3 Parquet datasets already exist for trades/events/markets",
            "If not present, run one-time bootstrap: convert local CSVs to typed Parquet",
            "Define canonical schemas for trades, events, and markets (types, nullable fields)",
            "Partition Parquet by time (year/month/day) and hash bucket for market_id",
            "Write raw Parquet to s3://<bucket>/raw/polymarket/<dataset>/",
            "Write curated Parquet to s3://<bucket>/curated/polymarket/<dataset>/",
            "Upload Parquet to S3 with deterministic file naming",
            "Generate verification report: row counts, min/max timestamps, distinct market_ids vs CSV",
            "Randomly sample rows and compare tx_hash/event_id between CSV and Parquet",
            "Fail bootstrap if verification does not pass",
            "Block deletion of local CSVs until verification passes",
            "Document repeatable bootstrap command and expected S3 layout"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Introduce INGEST_MODE and ANALYTICS_MODE runtime controls with safe transitions",
        "steps": [
            "Add global runtime configuration with INGEST_MODE = off | batched | live",
            "Add global runtime configuration with ANALYTICS_MODE = off | on_demand | live",
            "Ensure INGEST_MODE=live can coexist with ANALYTICS_MODE=off",
            "Persist current modes and last transition timestamp in ops metadata store",
            "Implement safe transitions with idempotent start/stop semantics"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Polymarket ingestion supports live, batched, and dormant operation",
        "steps": [
            "INGEST_MODE=off disables all ingestion except manual backfill",
            "INGEST_MODE=batched runs update_to_now() on a 5-minute cadence",
            "INGEST_MODE=live runs a long-lived ingest/consumer loop",
            "All ingestion paths archive events to S3 Parquet within 5 minutes",
            "Watermarks advance monotonically and support resumable backfill"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Wallet tracking with average-cost accounting and 5-minute mark-to-market",
        "steps": [
            "Maintain per-wallet per-market per-outcome positions with average cost",
            "Compute realized P&L on position size returning to zero",
            "Compute mark-to-market P&L every 5 minutes using last trade price in window",
            "Carry forward last known price if no trade in window; null until first print",
            "Ignore closed positions with zero P&L for win percentage calculations"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Win percentage and volume metrics",
        "steps": [
            "Win % is computed per closed position",
            "A win is realized P&L > 0; loss < 0; P&L == 0 ignored",
            "Volume is tracked as notional USDC traded",
            "Expose metrics at 1m, 1h, and 1d rollups"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "ClickHouse serving layer for rollups and recent drilldowns",
        "steps": [
            "Deploy single-node ClickHouse on EC2 with EBS mounted at /var/lib/clickhouse",
            "Create rollup tables: wallet_agg_1m, wallet_agg_1h, wallet_agg_1d",
            "Create wallet_positions_current table",
            "Optionally retain raw events in ClickHouse for last 30 days",
            "All historical raw data remains in S3 Parquet as source of truth"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "ClickHouse lifecycle management and backups",
        "steps": [
            "Support stopping and starting ClickHouse EC2 for cost control",
            "Implement on-demand analytics sessions with 120-minute idle TTL",
            "Automatically stop ClickHouse after TTL expires with no activity",
            "Take nightly EBS snapshots of ClickHouse data volume",
            "Document restore procedure from snapshot"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "LLM-driven analytics with conservative guardrails",
        "steps": [
            "Expose an analytics query interface callable by LLM and UI",
            "Restrict LLM queries to ClickHouse rollup tables only",
            "Restrict LLM queries to wallets present in the watchlist",
            "Apply default time window limit of 7 days unless explicitly overridden",
            "Enforce hard limits: max rows returned (10k), max execution time (30s), max memory",
            "Reject queries that attempt to access raw event tables",
            "Log executed SQL, parameters, execution time, and rows read to ops metadata store"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Sharp money v1 based on static watchlist",
        "steps": [
            "Initialize watchlist with addresses: 0x63ce342161250d705dc0b16df89036c8e5f9ba9a and 0x5388bc8cb72eb19a3bec0e8f3db6a77f7cd54d5a",
            "Track all trades for watchlisted wallets",
            "Generate in-app alerts when watchlisted wallets trade",
            "Persist watchlist and alert subscriptions in ops metadata store"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Alerting system with latency SLA",
        "steps": [
            "Generate in-app alerts for watchlist wallet trades",
            "Ensure alerts are emitted within 10â€“30 seconds in INGEST_MODE=live",
            "Deduplicate alerts by event id",
            "Apply per-rule cooldown window to prevent spam"
        ],
        "passes": false
    },
    {
        "category": "functional",
        "description": "Ops metadata and control plane",
        "steps": [
            "Use SQLite to store watermarks, run history, mode state, and TTLs",
            "Persist alert subscriptions and watchlists",
            "Record ingestion runs and analytics session lifecycle",
            "Enable future migration to Postgres without schema breakage"
        ],
        "passes": true
    },
    {
        "category": "functional",
        "description": "Carry-over: Each Dataset persists its market list (included/excluded) and can be edited later",
        "steps": [
            "Persist dataset definitions including included and excluded market ids",
            "Allow editing dataset definitions without rebuilding all data",
            "Ensure downstream jobs respect updated dataset definitions"
        ],
        "passes": true
    },
    {
        "category": "functional",
        "description": "Carry-over: Strategy confirmation screen before execution",
        "steps": [
            "Parse strategy parameters from user or LLM",
            "Display parsed strategy and parameters for confirmation",
            "Require explicit confirmation before execution"
        ],
        "passes": true
    },
    {
        "category": "functional",
        "description": "Carry-over: Backtest results visualization",
        "steps": [
            "Generate equity curve for backtest results",
            "Compute and display summary metrics",
            "Display trades table with pagination",
            "Store backtest artifacts for later viewing"
        ],
        "passes": true
    }
]