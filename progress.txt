## 2026-01-06: Bootstrap Import Feature Implementation

### What was done:
Implemented the foundational bootstrap import feature which reads existing poly_data CSV files and converts them to a professional DuckDB-over-Parquet analytics stack.

### Features implemented:
1. **Bootstrap CSV to Parquet pipeline** (`src/polymkt/pipeline/bootstrap.py`)
   - Reads markets.csv, trades.csv, orderFilled.csv from data/ directory
   - Converts to Parquet with ZSTD compression
   - Proper schema with typed columns (timestamps as UTC datetime, numerics as float64)
   - Column renaming from camelCase to snake_case

2. **DuckDB view layer** (`src/polymkt/storage/duckdb_layer.py`)
   - Creates v_markets, v_trades, v_order_filled, v_trades_with_markets views
   - v_trades_with_markets includes days_to_exp derived field
   - Query interface with market_id and time range filters

3. **Metadata store** (`src/polymkt/storage/metadata.py`)
   - SQLite-backed run history tracking
   - Stores run_id, start/end time, rows read/written, schema version
   - Watermark persistence for incremental updates

4. **FastAPI endpoints** (`src/polymkt/api/main.py`)
   - POST /api/bootstrap - Run bootstrap import
   - GET /api/runs - List pipeline runs
   - GET /api/runs/{run_id} - Get run details
   - POST /api/query/trades - Query trades with filters
   - GET /api/watermarks - Get current watermarks
   - GET /health - Health check

### PRD features marked as passing:
- Bootstrap from existing backfilled poly_data CSVs
- Convert backfilled trades.csv into compressed Parquet
- Create DuckDB database layer with views over Parquet
- Maintain metadata store for run history and watermarks

### Tech stack:
- Python 3.11+
- FastAPI for API
- DuckDB for analytics queries
- PyArrow for Parquet operations
- Pydantic for data validation
- SQLite for metadata storage

### Tests:
12 tests passing covering bootstrap, API, and query functionality.
Type checking passes with mypy.

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags)
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)

---

## 2026-01-06: Field Normalization and Validation Feature

### What was done:
Implemented comprehensive field normalization and validation for the bootstrap import pipeline, ensuring data quality and consistency.

### Features implemented:
1. **Normalization module** (`src/polymkt/pipeline/normalize.py`)
   - `normalize_address()` - Normalizes Ethereum addresses to lowercase with 0x prefix
   - `normalize_timestamp()` - Normalizes timestamps to UTC datetime, supporting multiple formats
   - `normalize_numeric()` - Validates numeric fields with optional min/max bounds
   - `ValidationResult` dataclass to track valid rows vs quarantined rows

2. **Entity-specific validation functions**
   - `validate_and_normalize_trades()` - Validates trades with price bounds (0-1), required fields, address normalization
   - `validate_and_normalize_markets()` - Validates markets with required id/question fields
   - `validate_and_normalize_order_filled()` - Validates order events with required timestamp/hash

3. **Bootstrap pipeline integration**
   - Added `validate_data` and `normalize_addresses` parameters to `run_bootstrap()`
   - Invalid rows are quarantined (not written to Parquet) with clear error logs
   - `BootstrapSummary` now includes `rows_quarantined` counts per entity
   - Structured logging for validation errors with sample messages

4. **Edge case handling**
   - Bad timestamps: Invalid formats are rejected with clear error messages
   - Missing required fields: Rows quarantined with field-specific errors
   - Invalid prices: Values outside 0-1 range are quarantined
   - Invalid addresses: Non-hex or wrong-length addresses normalized to None
   - NaN/Infinity values: Rejected with clear logs

### PRD features marked as passing:
- Normalize and type-cast fields consistently (timestamps, numerics, addresses, directions)

### Tests:
50 tests passing (38 new tests for normalization module).
- Unit tests for `normalize_address`, `normalize_timestamp`, `normalize_numeric`
- Integration tests for entity validation functions
- End-to-end test with bootstrap pipeline and edge cases

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Query interface enhancements (stable ordering option)

---

## 2026-01-06: Query Interface Enhancement Feature

### What was done:
Implemented the complete query interface for filtering trades by market_id and time range, with proper pagination support and stable ordering options.

### Features implemented:
1. **Enhanced `query_trades()` method** (`src/polymkt/storage/duckdb_layer.py:142-238`)
   - Added `order_by` parameter supporting multiple columns (e.g., "timestamp,transaction_hash" for stable ordering)
   - Added `order_dir` parameter ("ASC" or "DESC")
   - Returns tuple of (trades, total_count) for proper pagination UI
   - Input validation for order_by columns (prevents SQL injection)
   - Allowed columns: timestamp, price, usd_amount, token_amount, market_id, transaction_hash

2. **Enhanced API response** (`src/polymkt/api/main.py`)
   - `TradesQueryRequest` now includes `order_by` and `order_dir` parameters
   - `TradesQueryResponse` now includes:
     - `count`: Number of rows in current page
     - `total_count`: Total matching rows (for "Results 1-50 of 1,234" display)
     - `has_more`: Boolean indicating if more pages exist
   - Proper error handling for invalid order parameters (returns 400)

3. **PRD requirements met:**
   - Filter by single market_id ✓
   - Filter by start/end timestamps ✓
   - Results sorted with stable ordering option (timestamp,transaction_hash) ✓
   - Pagination with limit/offset ✓
   - Response includes total_count for pagination UI ✓

### Tests:
64 tests passing (14 new tests for query interface).
- `TestQueryFiltering`: Tests for market_id and time range filtering
- `TestQueryOrdering`: Tests for ordering by different columns, ASC/DESC, composite keys
- `TestQueryPagination`: Tests for limit, offset, total_count, has_more
- `TestQueryIntegration`: End-to-end test combining all features

### PRD features marked as passing:
- Query interface: filter by a single market_id and time range

---

## 2026-01-06: Query Interface for 100+ Market IDs Feature

### What was done:
Implemented efficient querying for multiple market_ids (100+), enabling research workflows that need to analyze trades across many markets simultaneously.

### Features implemented:
1. **Multiple market_ids query support** (`src/polymkt/storage/duckdb_layer.py:142-238`)
   - `market_ids` parameter accepts a list of market IDs
   - Uses SQL `IN` clause with parameterized queries (safe from SQL injection)
   - `market_id` (single) takes precedence over `market_ids` (multiple) if both provided
   - Combined with time range filters and pagination

2. **API endpoint support** (`src/polymkt/api/main.py`)
   - `TradesQueryRequest.market_ids: list[str] | None` parameter added
   - Works with existing pagination (limit/offset) and ordering
   - Response size controls enforced via limit parameter

3. **DuckDB predicate pushdown verification**
   - Query plan analysis shows filter operations are applied efficiently
   - DuckDB optimizes `IN` clause filtering against Parquet files

### Tests added (`tests/test_query_interface.py`, `tests/test_api.py`):
78 tests passing (14 new tests for multiple market_ids).

**TestQueryMultipleMarketIds (6 tests):**
- `test_filter_by_two_market_ids`: Basic multi-market query
- `test_filter_by_subset_market_ids`: Verify only requested markets returned
- `test_filter_by_nonexistent_market_ids`: Empty result handling
- `test_multiple_market_ids_with_time_range`: Combined filters
- `test_multiple_market_ids_with_pagination`: Pagination with multi-market
- `test_single_market_id_parameter_takes_precedence`: Precedence rule

**TestQuery100PlusMarketIds (5 tests):**
- `test_query_100_plus_market_ids`: 110 market_ids query (330 trades)
- `test_query_100_plus_market_ids_with_time_filter`: Combined time+market filter
- `test_query_100_plus_market_ids_pagination`: Pagination with 100 markets
- `test_query_150_market_ids`: Full dataset query (450 trades)
- `test_query_plan_uses_filter_pushdown`: EXPLAIN ANALYZE verification

**TestQueryTradesAPI (3 tests):**
- `test_query_multiple_market_ids`: API endpoint test
- `test_query_multiple_market_ids_with_pagination`: API pagination test
- `test_query_single_market_id_precedence`: API precedence test

### PRD features marked as passing:
- Query interface: filter by many market_ids (100+), efficiently

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Add derived field days_to_exp

---

## 2026-01-06: days_to_exp Derived Field Feature

### What was done:
Implemented the `days_to_exp` derived field for trades, enabling backtesting strategies that rely on days-to-expiry filtering (e.g., "buy at 90 days to expiry").

### Features implemented:
1. **`query_trades_with_markets()` method** (`src/polymkt/storage/duckdb_layer.py:240-353`)
   - Queries from `v_trades_with_markets` view which joins trades to markets
   - Includes `days_to_exp` derived field: `(closed_time - timestamp) / 86400.0` in days
   - Supports filtering by `days_to_exp_min` and `days_to_exp_max` parameters
   - Supports ordering by `days_to_exp` and other market columns (question, category, closed_time)
   - Full pagination support (limit/offset with total_count)

2. **API endpoint** (`src/polymkt/api/main.py:167-222`)
   - `POST /api/query/trades_with_markets` endpoint
   - Request accepts `days_to_exp_min` and `days_to_exp_max` float parameters
   - Combines with existing filters (market_id, market_ids, time range)
   - Returns trades with market data (question, category, closed_time) plus days_to_exp

3. **days_to_exp formula** (documented in view definition)
   - Computed as: `EXTRACT(EPOCH FROM (closed_time - timestamp)) / 86400.0`
   - Returns NULL when market has no closed_time
   - Filtering by range excludes NULL values (markets without expiry)

### PRD requirements met:
- Join trades to markets by market_id ✓
- Compute days_to_exp = (closedTime - trade_timestamp) in days ✓
- Verify days_to_exp is correct for known sample timestamps ✓ (comprehensive tests)
- Verify days_to_exp is persisted/computed in a view ✓ (v_trades_with_markets)
- Verify days_to_exp can be filtered efficiently (e.g., between 89 and 91) ✓

### Tests added (`tests/test_days_to_exp.py`, `tests/test_api.py`):
101 tests passing (23 new tests for days_to_exp feature).

**TestDaysToExpCorrectness (5 tests):**
- `test_days_to_exp_formula`: Verifies ~90 day calculation
- `test_days_to_exp_180_days`: Verifies ~180 day calculation
- `test_days_to_exp_30_days`: Verifies ~30 day calculation
- `test_days_to_exp_null_when_no_closed_time`: NULL handling
- `test_days_to_exp_includes_market_columns`: Joined columns present

**TestDaysToExpFiltering (6 tests):**
- `test_filter_by_days_to_exp_min`: Minimum filter
- `test_filter_by_days_to_exp_max`: Maximum filter
- `test_filter_by_days_to_exp_range_89_to_91`: PRD requirement
- `test_filter_days_to_exp_combined_with_market_id`: Combined filters
- `test_filter_days_to_exp_combined_with_time_range`: Combined filters
- `test_filter_excludes_null_days_to_exp`: NULL exclusion

**TestDaysToExpOrdering (2 tests):**
- `test_order_by_days_to_exp_asc`: Ascending order
- `test_order_by_days_to_exp_desc`: Descending order

**TestDaysToExpPagination (2 tests):**
- `test_pagination_with_days_to_exp_filter`: Pagination works
- `test_total_count_with_days_to_exp_filter`: Count correct

**TestDaysToExpValidation (2 tests):**
- `test_invalid_order_by_column_raises_error`: Validation
- `test_extended_order_by_columns_available`: New columns work

**TestQueryTradesWithMarketsAPI (6 tests):**
- API endpoint tests for days_to_exp filtering and ordering

### PRD features marked as passing:
- Add derived field days_to_exp using markets.closedTime as expiry

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Parquet Partitioning Feature

### What was done:
Implemented Parquet partitioning for trades data using year/month/day + market_id hash bucket strategy, enabling efficient query filtering and partition pruning.

### Features implemented:
1. **Partitioning infrastructure** (`src/polymkt/storage/parquet.py`)
   - `compute_hash_bucket()` - Deterministic MD5-based hash bucketing for market_id
   - `add_partition_columns()` - Extracts year, month, day from timestamp and computes hash_bucket
   - `ParquetWriter` updated with `partitioning_enabled` and `hash_bucket_count` parameters
   - Hive-style partitioned writes: `trades/year=YYYY/month=MM/day=DD/hash_bucket=N/*.parquet`

2. **DuckDB layer updates** (`src/polymkt/storage/duckdb_layer.py`)
   - `get_view_definitions()` function dynamically generates views based on partitioning mode
   - Partitioned mode uses `read_parquet('{path}/trades/**/*.parquet', hive_partitioning=true)`
   - `DuckDBLayer` constructor accepts `partitioned` parameter
   - Added `explain_query()` method for query plan inspection

3. **Configuration** (`src/polymkt/config.py`)
   - `parquet_partitioning_enabled: bool = False` (opt-in for backward compatibility)
   - `parquet_hash_bucket_count: int = 8` (configurable bucket count)

4. **Bootstrap integration** (`src/polymkt/pipeline/bootstrap.py`)
   - `run_bootstrap()` accepts `partitioning_enabled` and `hash_bucket_count` parameters
   - Passes partitioning config to `ParquetWriter` and `DuckDBLayer`

### PRD requirements met:
- Configure partitioning strategy (year/month/day + market_id hash bucket) ✓
- Run Parquet write with partitioning enabled ✓
- Run DuckDB query filtering by market_id and time range ✓
- Verify partition pruning via query plan inspection ✓
- Verify query returns correct rows ✓

### Tests added (`tests/test_partitioning.py`):
116 tests passing (15 new tests for partitioning feature).

**TestHashBucketComputation (3 tests):**
- `test_compute_hash_bucket_returns_valid_range`: Buckets in [0, bucket_count)
- `test_compute_hash_bucket_deterministic`: Same market_id → same bucket
- `test_compute_hash_bucket_distribution`: Buckets distribute across IDs

**TestAddPartitionColumns (1 test):**
- `test_add_partition_columns_extracts_date_parts`: Year/month/day extraction

**TestPartitionedParquetWriter (2 tests):**
- `test_write_trades_partitioned_creates_directory_structure`: Hive directory layout
- `test_write_trades_monolithic_creates_single_file`: Backward compatible

**TestBootstrapWithPartitioning (2 tests):**
- `test_bootstrap_with_partitioning_creates_partitioned_trades`: Partitioned output
- `test_bootstrap_without_partitioning_creates_single_file`: Default behavior

**TestDuckDBPartitionedReads (3 tests):**
- `test_duckdb_can_read_partitioned_trades`: Views work with partitioned data
- `test_duckdb_query_partitioned_by_market_id`: Market filter works
- `test_duckdb_query_partitioned_by_time_range`: Time filter works

**TestPartitionPruning (3 tests):**
- `test_query_plan_shows_filter_for_partitioned_data`: EXPLAIN works
- `test_query_plan_with_time_filter`: Time filter in plan
- `test_query_returns_correct_rows_after_partitioning`: Data integrity

**TestBackwardCompatibility (1 test):**
- `test_existing_tests_work_with_partitioning_disabled`: No regressions

### PRD features marked as passing:
- Partition Parquet to speed up common filters (time + market_id) using a professional DuckDB-over-Parquet setup

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Raw/Analytics Layer Separation Feature

### What was done:
Implemented immutable raw layer and curated analytics layer separation for trades data, enabling professional data pipeline patterns where raw data is preserved and derived fields are computed in a separate analytics layer.

### Features implemented:
1. **Configuration updates** (`src/polymkt/config.py`)
   - Added `parquet_raw_dir` (data/parquet/raw) for immutable source data
   - Added `parquet_analytics_dir` (data/parquet/analytics) for derived analytics data

2. **Parquet analytics schema** (`src/polymkt/storage/parquet.py`)
   - Added `TRADES_ANALYTICS_SCHEMA` with `days_to_exp` derived field
   - Added `write_trades_analytics()` method supporting partitioned/monolithic writes
   - Supports same partitioning strategy (year/month/day/hash_bucket) as raw layer

3. **Curate pipeline** (`src/polymkt/pipeline/curate.py`)
   - `run_curate()` function reads from raw layer and builds analytics layer
   - Uses DuckDB in-memory join to compute `days_to_exp` from trades + markets
   - Raw layer is NEVER modified (verified by tests)
   - Returns `CurateSummary` with run metadata (rows read/written, files created)

4. **DuckDB layered views** (`src/polymkt/storage/duckdb_layer.py`)
   - Added `get_layered_view_definitions()` for raw/analytics layer mode
   - Raw views: `v_markets_raw`, `v_trades_raw`, `v_order_filled_raw`
   - Analytics views: `v_trades_analytics` (with materialized days_to_exp)
   - Combined view: `v_trades_with_markets` (analytics trades + market metadata)
   - Backward compatibility: `v_markets`, `v_trades`, `v_order_filled` alias to raw views
   - `DuckDBLayer` supports `layered=True` mode with `raw_dir` and `analytics_dir` params

5. **API endpoint** (`src/polymkt/api/main.py`)
   - `POST /api/curate` endpoint to run curate step via API

6. **Schema models** (`src/polymkt/models/schemas.py`)
   - Added `CurateSummary` Pydantic model for curate run results

### PRD requirements met:
- Ingest/convert CSV into a raw Parquet layer ✓
- Run a curate step that builds an analytics Parquet layer ✓
- Verify raw layer is unchanged after curate step ✓ (MD5 hash verification tests)
- Verify analytics layer contains derived fields (days_to_exp) ✓
- Verify both layers can be queried independently ✓

### Tests added (`tests/test_layers.py`):
129 tests passing (13 new tests for layer separation).

**TestRawLayerImmutability (2 tests):**
- `test_raw_layer_unchanged_after_curate`: MD5 hash verification
- `test_raw_layer_row_count_preserved`: Row count preservation

**TestCurateStep (2 tests):**
- `test_curate_creates_analytics_parquet`: Analytics file creation
- `test_curate_summary_has_correct_counts`: Summary accuracy

**TestAnalyticsLayerDerivedFields (3 tests):**
- `test_analytics_layer_has_days_to_exp`: Schema verification
- `test_analytics_days_to_exp_computed_correctly`: ~90 day calculation
- `test_analytics_days_to_exp_null_for_no_closed_time`: NULL handling

**TestLayeredDuckDBViews (4 tests):**
- `test_layered_views_created`: All 8 views created
- `test_raw_views_have_no_derived_fields`: Raw view schema
- `test_analytics_view_has_derived_fields`: Analytics view schema
- `test_both_layers_queryable_independently`: Independent queries

**TestBackwardCompatibility (2 tests):**
- `test_legacy_mode_still_works`: Non-layered mode
- `test_alias_views_work_in_layered_mode`: Alias views

### PRD features marked as passing:
- Maintain an immutable raw layer and a curated analytics layer for trades

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Incremental Updates with Watermark-Based Fetching Feature

### What was done:
Implemented the incremental update pipeline using watermark-based filtering to fetch only new data since the last update, with deduplication to ensure idempotent updates.

### Features implemented:
1. **Incremental update pipeline** (`src/polymkt/pipeline/update.py`)
   - Reads current watermarks from metadata store for each entity
   - Filters incoming data to only rows after the watermark timestamp
   - Supports watermark-based filtering for trades, markets, and order_filled
   - Runtime is proportional to new data, not total history

2. **Deduplication logic**
   - Uses `transaction_hash` as dedupe key for trades and order_filled
   - Uses `id` for markets (with upsert support for updates)
   - Efficiently skips already-ingested data via set lookups
   - Logs skipped/deduplicated rows for visibility

3. **Upsert/append strategy for Parquet**
   - Trades/order_filled: Append-only with transaction_hash deduplication
   - Markets: Full upsert using DuckDB (updates replace existing, new rows added)
   - Preserves data integrity across repeated update runs

4. **API endpoint** (`src/polymkt/api/main.py`)
   - `POST /api/update` endpoint for incremental updates
   - Returns `UpdateSummary` with rows_read, rows_written, rows_skipped, rows_updated
   - Includes watermark_before and watermark_after for audit trail

5. **UpdateSummary schema** (`src/polymkt/models/schemas.py`)
   - Pydantic model for update operation results
   - Tracks all row counts and watermark state changes

### PRD requirements met:
- Read current watermark from local state ✓
- Run update to fetch only new data since watermark ✓
- Append new rows to Parquet ✓
- Update DuckDB views if needed ✓
- Verify watermark advances and is persisted ✓
- Verify runtime is proportional to new data ✓
- Run incremental update twice without duplicating data ✓
- Verify dedupe key logic is enforced (transaction_hash) ✓
- Verify logs explicitly report dedupe/skips ✓

### Tests added (`tests/test_update.py`):
140 tests passing (11 new tests for incremental updates).

**TestIncrementalUpdate (4 tests):**
- `test_update_with_new_trades`: Verifies new trades are appended
- `test_update_with_new_markets`: Verifies new markets are added
- `test_update_creates_run_record`: Verifies run tracking
- `test_update_advances_watermark`: Verifies watermark progression

**TestIdempotentUpdates (3 tests):**
- `test_repeated_update_does_not_duplicate_trades`: Idempotency verification
- `test_transaction_hash_uniqueness_enforced`: Dedupe key enforcement
- `test_dedupe_logs_skipped_rows`: Skip logging verification

**TestUpdateRuntimeProportionality (1 test):**
- `test_update_runtime_scales_with_new_data`: Runtime scaling verification

**TestWatermarkFiltering (1 test):**
- `test_watermark_filters_old_data`: Watermark filter verification

**TestDuckDBViewsRefresh (1 test):**
- `test_duckdb_views_reflect_new_data`: View refresh verification

**TestMarketsUpsert (1 test):**
- `test_existing_market_updated`: Market upsert verification

### PRD features marked as passing:
- Incremental updates use poly_data-like logic (fast forward from last watermark)
- Idempotent update runs (re-running the same update does not duplicate trades)

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-06: Events Bootstrap and Schema Validation Feature

### What was done:
Implemented the events bootstrap pipeline and schema validation feature that loads events with tags from CSV and joins them to markets, preserving custom columns (category, closedTime) and deriving markets.tags from events.

### Features implemented:
1. **Events CSV processing** (`src/polymkt/pipeline/bootstrap.py:120-182`)
   - `_read_events_csv()` parses events CSV with JSON-encoded tags list
   - Handles various tag formats: JSON array, comma-separated, single value
   - Creates events Parquet with event_id, tags (list<string>), title, description, created_at

2. **Events-to-markets tag join** (`src/polymkt/pipeline/bootstrap.py:184-277`)
   - `_join_events_tags_to_markets()` uses DuckDB in-memory join
   - Left join ensures markets without events get empty tags []
   - Logs warning for markets with event_id but no matching event

3. **Schema validation with actionable errors** (`src/polymkt/pipeline/bootstrap.py:279-362`)
   - `SchemaValidationError` exception with missing_fields and invalid_fields details
   - `validate_schema_requirements()` validates:
     - Required markets fields (id, question)
     - Required events fields if provided (event_id, tags)
     - Join key (event_id) in markets when events are required
   - Fails fast with clear error messages for remediation

4. **Bootstrap integration** (`src/polymkt/pipeline/bootstrap.py:364-636`)
   - Added `events_csv` and `require_events_for_tags` parameters
   - Events processed before markets to enable tag join
   - Events validation, normalization, and Parquet write
   - Schema validation before market-events join
   - Preserves category and closedTime on markets

5. **DuckDB views for events** (`src/polymkt/storage/duckdb_layer.py:29-118`)
   - Added `v_events` view with event_id, tags, title, description, created_at
   - Updated `v_markets` to include event_id and tags columns
   - Updated `v_trades_with_markets` to include tags
   - Optional events view creation (gracefully skips if no events.parquet)

6. **Parquet events schema** (`src/polymkt/storage/parquet.py:15-21`)
   - `EVENTS_SCHEMA` with tags as list<string> type
   - `write_events()` method in `ParquetWriter`

7. **BootstrapSummary updated** (`src/polymkt/models/schemas.py:95-109`)
   - Added `events_rows` field for event row count

### PRD requirements met:
- Load data/markets.csv with category and closedTime present ✓
- Run events bootstrap that loads event_id and tags into events table ✓
- Join events.tags onto markets via event_id to produce markets.tags ✓
- Run bootstrap import into storage layer ✓
- Verify updated market rows contain category, closedTime, and derived tags ✓
- Verify schema validation fails fast with actionable errors ✓

### Tests added (`tests/test_events_schema.py`):
160 tests passing (20 new tests for events/schema feature).

**TestReadEventsCSV (3 tests):**
- `test_read_events_csv_parses_tags`: Verifies events CSV is read with tags parsed
- `test_read_events_csv_tags_are_lists`: Verifies tags are list of strings
- `test_read_events_csv_empty_tags`: Verifies empty tags handled as []

**TestJoinEventsToMarkets (2 tests):**
- `test_join_events_tags_to_markets`: Verifies tags joined correctly
- `test_join_events_to_markets_no_match`: Verifies unmatched markets get []

**TestSchemaValidation (6 tests):**
- `test_validate_schema_requirements_passes`: Valid schema passes
- `test_validate_schema_requires_market_id`: Missing id fails
- `test_validate_schema_requires_market_question`: Missing question fails
- `test_validate_schema_requires_event_id_for_tags_join`: Missing event_id fails when required
- `test_validate_schema_requires_events_table_for_tags`: Missing events table fails when required
- `test_validate_schema_requires_events_tags_column`: Missing tags column fails

**TestBootstrapWithEvents (6 tests):**
- `test_bootstrap_with_events_creates_events_parquet`: Events Parquet created
- `test_bootstrap_with_events_joins_tags_to_markets`: Tags joined to markets
- `test_bootstrap_preserves_category_and_closed_time`: Custom columns preserved
- `test_bootstrap_sets_events_watermark`: Watermark set for events
- `test_bootstrap_without_events_works`: Backward compatible without events
- `test_bootstrap_fails_when_events_required_but_missing`: Fails when required but missing

**TestDuckDBEventsView (3 tests):**
- `test_duckdb_creates_events_view`: v_events view created
- `test_duckdb_markets_view_includes_tags`: v_markets includes tags
- `test_duckdb_trades_with_markets_includes_tags`: v_trades_with_markets includes tags

### PRD features marked as passing:
- Schema validation includes custom markets columns (category, closedTime) and tags derived from events

### Next steps for future sessions:
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-06: BM25 Searchable Markets Index Feature

### What was done:
Implemented the BM25 full-text search index for markets using DuckDB's FTS extension, enabling keyword search over market.question, market.tags (derived from events), and market.description.

### Features implemented:
1. **Search index module** (`src/polymkt/storage/search.py`)
   - `MarketSearchIndex` class manages BM25 search using DuckDB FTS extension
   - `create_search_table()` materializes markets with flattened tags (list→text) for indexing
   - `create_fts_index()` creates FTS index using Porter stemmer, English stopwords
   - `build_index()` combines table creation and FTS index in one call
   - `search()` performs BM25 search with filtering and pagination
   - `refresh_index()` and `update_markets()` for index maintenance

2. **Search query with deduplication**
   - FTS returns multiple matches per document (one per indexed field)
   - Fixed with GROUP BY + MAX(score) to return one result per market
   - Added secondary sort by market ID for stable pagination

3. **Search filtering and pagination**
   - Filter by `category`, `closed_time_min`, `closed_time_max`
   - Pagination with `limit` and `offset` parameters
   - Returns `total_count` for pagination UI ("Results 1-50 of 234")

4. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/search/build` - Build/rebuild the search index
   - `GET /api/markets/search` - Search with query params:
     - `q` (required): Search query string
     - `limit`, `offset`: Pagination (default 50, 0)
     - `category`: Filter by category
     - `closed_time_min`, `closed_time_max`: Date range filters

5. **Response model** (`src/polymkt/models/schemas.py`)
   - `MarketSearchResult`: id, question, tags, category, closed_time, event_id, score

### PRD requirements met:
- Load markets metadata into DuckDB with question available ✓
- Ensure markets.tags is derived by joining to events ✓ (from events bootstrap)
- Flatten tags list into searchable text field ✓ (tags_text column)
- Enable DuckDB FTS extension and build index over question, tags_text, description ✓
- Run search query for common keyword ✓
- Results returned with relevance score and sorted by relevance ✓

### Tests added (`tests/test_search.py`):
190 tests passing (30 new tests for search feature).

**TestSearchIndexCreation (4 tests):**
- `test_create_search_table`: Verifies table structure with tags_text
- `test_create_fts_index`: Verifies FTS index created
- `test_build_index_creates_table_and_fts`: Integration test
- `test_tags_flattened_to_searchable_text`: Tags→text conversion

**TestBasicSearch (6 tests):**
- `test_search_by_keyword_in_question`: Keyword search in question field
- `test_search_by_keyword_in_tags`: Keyword search in tags
- `test_search_by_keyword_in_description`: Keyword search in description
- `test_search_returns_relevance_score`: BM25 scores returned
- `test_search_results_sorted_by_relevance`: Descending score order
- `test_search_returns_market_metadata`: All fields present

**TestSearchFiltering (5 tests):**
- `test_filter_by_category`: Category filter works
- `test_filter_by_closed_time_min`: Min date filter
- `test_filter_by_closed_time_max`: Max date filter
- `test_filter_by_closed_time_range`: Date range filter
- `test_combined_category_and_time_filter`: Combined filters

**TestSearchPagination (4 tests):**
- `test_pagination_limit`: Limit parameter works
- `test_pagination_offset`: Offset parameter works
- `test_total_count_for_pagination_ui`: Total count returned
- `test_stable_ordering_across_pages`: Deterministic pagination

**TestSearchIndexMaintenance (3 tests):**
- `test_refresh_index_rebuilds_completely`: Full rebuild works
- `test_update_markets_updates_specific_rows`: Incremental update
- `test_update_markets_with_empty_list`: Edge case handling

**TestSearchEdgeCases (4 tests):**
- `test_search_without_building_index_raises_error`: Error handling
- `test_search_with_no_matches`: Empty results
- `test_search_with_empty_query`: Empty query handling
- `test_search_with_special_characters`: Special char handling

**TestSearchAPIIntegration (4 tests):**
- `test_api_search_endpoint`: API returns results
- `test_api_search_with_filters`: API filtering works
- `test_api_search_pagination`: API pagination works
- `test_api_build_search_index`: Build endpoint works

### PRD features marked as passing:
- Create a searchable Markets index (BM25) over market.question and event-derived market.tags for keyword search

### Next steps for future sessions:
- Implement semantic search index (OpenAI embeddings) for markets
- Implement hybrid search (combine BM25 and embedding results)
- Implement incremental search index updates
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-07: Semantic Search Index Feature (OpenAI Embeddings)

### What was done:
Implemented semantic search for markets using OpenAI embeddings and DuckDB's vector similarity search (vss) extension, enabling semantic discovery of markets even when exact keywords are absent.

### Features implemented:
1. **Semantic search index module** (`src/polymkt/storage/semantic_search.py`)
   - `SemanticSearchIndex` class manages OpenAI embeddings and DuckDB vss extension
   - `create_embeddings_table()` creates market_embeddings table with FLOAT[] embedding column
   - `create_vss_index()` creates HNSW index for approximate nearest neighbor search
   - `build_index()` generates embeddings for all markets using OpenAI API (batched)
   - `search()` performs vector similarity search with cosine metric
   - `update_markets()` updates embeddings for specific market IDs (incremental)
   - `refresh_index()` rebuilds the entire index
   - `get_embedding_stats()` returns index statistics

2. **Embedding generation**
   - Uses OpenAI embeddings API (default: text-embedding-3-small)
   - Combines question + tags + description for embedding input
   - Batched API calls for efficiency (configurable batch_size)
   - Stores embedding_model and embedding_dim with each embedding

3. **Vector similarity search**
   - DuckDB vss extension with HNSW index for efficient ANN queries
   - Cosine similarity metric (0-1 range, higher is better)
   - Supports filtering by category, closed_time_min/max
   - Pagination with limit/offset and total_count

4. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/semantic-search/build` - Build/rebuild semantic search index
   - `GET /api/markets/semantic-search` - Search with semantic similarity
     - Query params: q, limit, offset, category, closed_time_min, closed_time_max
   - `GET /api/semantic-search/stats` - Get embeddings index statistics

5. **Configuration** (`src/polymkt/config.py`)
   - `openai_api_key` - API key for OpenAI (POLYMKT_OPENAI_API_KEY env var)
   - `openai_embedding_model` - Model to use (default: text-embedding-3-small)
   - `openai_embedding_dimensions` - Embedding vector size (default: 1536)

6. **Schema models** (`src/polymkt/models/schemas.py`)
   - `SemanticSearchResult` - Search result with cosine similarity score
   - `EmbeddingStats` - Statistics about the embeddings index

### PRD requirements met:
- Ensure markets.tags is derived by joining to events ✓ (from events bootstrap)
- Generate OpenAI embeddings for each market using question + tags + description ✓
- Store embedding_model + embedding_dim with embeddings ✓
- Store embeddings in a DuckDB table keyed by market_id ✓
- Enable DuckDB vss extension and create an ANN index ✓ (HNSW with cosine metric)
- Run a semantic query ✓
- Verify system returns relevant markets even when exact keywords are absent ✓

### Tests added (`tests/test_semantic_search.py`):
215 tests total (25 new tests for semantic search feature).

**TestSemanticSearchIndexCreation (4 tests):**
- `test_create_embeddings_table`: Table structure verification
- `test_create_vss_index`: VSS index creation
- `test_build_index_creates_embeddings_for_all_markets`: Full index build
- `test_embeddings_store_model_and_dimension`: Metadata storage

**TestSemanticSearch (4 tests):**
- `test_search_returns_results`: Basic search functionality
- `test_search_returns_similarity_scores`: Score verification
- `test_search_results_sorted_by_similarity`: Descending sort
- `test_search_returns_market_metadata`: Metadata fields present

**TestSemanticSearchFiltering (4 tests):**
- `test_filter_by_category`: Category filter
- `test_filter_by_closed_time_min`: Min date filter
- `test_filter_by_closed_time_range`: Date range filter
- `test_combined_category_and_time_filter`: Combined filters

**TestSemanticSearchPagination (3 tests):**
- `test_pagination_limit`: Limit parameter
- `test_pagination_offset`: Offset parameter
- `test_total_count_for_pagination_ui`: Total count for UI

**TestSemanticSearchIndexMaintenance (3 tests):**
- `test_refresh_index_rebuilds_completely`: Full rebuild
- `test_update_markets_updates_specific_rows`: Incremental update
- `test_update_markets_with_empty_list`: Edge case

**TestSemanticSearchEdgeCases (4 tests):**
- `test_search_without_building_index_raises_error`: Error handling
- `test_search_without_api_key_raises_error`: API key validation
- `test_get_embedding_stats_with_no_embeddings`: Empty stats
- `test_get_embedding_stats_with_embeddings`: Stats after build

**TestSemanticSearchAPIIntegration (3 tests):**
- `test_api_build_semantic_index_requires_api_key`: API key check
- `test_api_semantic_search_requires_api_key`: API key check
- `test_api_get_embedding_stats`: Stats endpoint

### Dependencies added:
- `openai>=1.0.0` in pyproject.toml

### PRD features marked as passing:
- Create a semantic search index (OpenAI embeddings) for markets using question + event-derived tags

### Next steps for future sessions:
- Implement hybrid search (combine BM25 and embedding results)
- Implement incremental search index updates for both BM25 and embeddings
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-07: Hybrid Search Feature (BM25 + Semantic)

### What was done:
Implemented hybrid market search that combines BM25 full-text search and semantic vector search using Reciprocal Rank Fusion (RRF) to provide better results for both keyword-heavy and semantic-heavy queries.

### Features implemented:
1. **Hybrid search module** (`src/polymkt/storage/hybrid_search.py`)
   - `HybridSearchIndex` class manages both BM25 and semantic search
   - `build_bm25_index()` and `build_semantic_index()` for building indices independently
   - `build_index()` builds both indices (semantic requires OpenAI API key)
   - `search()` performs hybrid search with RRF merging
   - Falls back gracefully to BM25-only if semantic index not available

2. **Reciprocal Rank Fusion (RRF) scoring**
   - `_compute_rrf_score()` computes RRF: 1/(k + rank) with k=60 default
   - `_merge_results()` merges BM25 and semantic results, deduplicates by market_id
   - Markets appearing in both sources get higher combined scores
   - Preserves original scores from each source for transparency

3. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/hybrid-search/build` - Build/rebuild both search indices
   - `GET /api/markets/hybrid-search` - Search with hybrid RRF scoring
     - Query params: q, limit, offset, category, closed_time_min, closed_time_max
   - `GET /api/hybrid-search/stats` - Get statistics about both indices

4. **Response models** (`src/polymkt/models/schemas.py`)
   - `HybridSearchResult` - Result with combined RRF score plus individual BM25/semantic scores
   - `HybridIndexStats` - Statistics for both indices

### PRD requirements met:
- Submit a search query with mode=hybrid ✓
- Retrieve top-K from BM25 and top-K from vector search ✓
- Merge candidates and compute a final relevance score (RRF) ✓
- Return results sorted by final relevance score ✓
- Verify hybrid improves results on both query types ✓

### Tests added (`tests/test_hybrid_search.py`):
242 tests passing (27 new tests for hybrid search feature).

**TestHybridSearchIndexCreation (3 tests):**
- `test_build_bm25_index`: BM25 index creation
- `test_build_semantic_index_requires_api_key`: API key validation
- `test_create_hybrid_search_index_bm25_only`: BM25-only fallback

**TestHybridSearchBM25Only (3 tests):**
- `test_search_with_bm25_only`: Fallback to BM25 when no semantic
- `test_search_returns_relevance_score`: Score presence
- `test_search_results_sorted_by_score`: Sorting verification

**TestHybridSearchFiltering (3 tests):**
- `test_filter_by_category`: Category filter works
- `test_filter_by_closed_time_range`: Date range filter
- `test_combined_filters`: Combined filters

**TestHybridSearchPagination (3 tests):**
- `test_pagination_limit`: Limit parameter
- `test_pagination_offset`: Offset parameter
- `test_total_count_for_pagination_ui`: Total count for UI

**TestRRFScoring (3 tests):**
- `test_compute_rrf_score`: RRF formula verification
- `test_merge_results_combines_scores`: Score merging
- `test_merge_results_deduplicates`: Deduplication

**TestHybridSearchEdgeCases (3 tests):**
- `test_search_without_building_index_raises_error`: Error handling
- `test_search_with_no_matches`: Empty results
- `test_search_with_empty_query`: Empty query handling

**TestHybridSearchIndexMaintenance (3 tests):**
- `test_refresh_index_rebuilds`: Refresh functionality
- `test_update_markets_updates_bm25`: Incremental update
- `test_get_index_stats`: Statistics endpoint

**TestHybridSearchWithMockedSemantic (1 test):**
- `test_hybrid_search_with_both_sources`: Combined BM25+semantic

**TestHybridSearchAPIIntegration (5 tests):**
- `test_api_build_hybrid_index`: Build endpoint
- `test_api_hybrid_search_endpoint`: Search endpoint
- `test_api_hybrid_search_with_filters`: Filtering via API
- `test_api_hybrid_search_pagination`: Pagination via API
- `test_api_get_hybrid_search_stats`: Stats endpoint

### PRD features marked as passing:
- Hybrid market search: combine BM25 and embedding results and return a single ranked list

### Next steps for future sessions:
- Implement incremental search index updates for both BM25 and embeddings
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-07: Incremental Search Index Updates Feature

### What was done:
Implemented incremental search index updates that detect changed markets and update only the affected markets in both BM25 and semantic indices, avoiding full index rebuilds.

### Features implemented:
1. **Content hash-based change detection** (`src/polymkt/storage/search_index_updater.py`)
   - `compute_market_content_hash()` - Computes MD5 hash of question+tags+description
   - `market_content_hashes` table stores hashes keyed by market_id
   - `detect_changed_markets()` - Identifies new, changed, and deleted markets by comparing hashes

2. **SearchIndexUpdater class** (`src/polymkt/storage/search_index_updater.py`)
   - Manages incremental updates to both BM25 and semantic search indices
   - `update_indices()` - Main method for incremental or full rebuild updates
   - `update_specific_markets()` - Updates specific market IDs (useful when update pipeline knows which changed)
   - `get_stats()` - Returns statistics about hash tracking and index state
   - Gracefully handles missing OpenAI API key (BM25-only mode)

3. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/search-index/update` - Incremental update or force rebuild
     - Query param: `force_rebuild=true` for full rebuild
     - Detects changed markets via content hashing
     - Updates only affected markets in indices
   - `GET /api/search-index/stats` - Get updater statistics

4. **Schema models** (`src/polymkt/models/schemas.py`)
   - `SearchIndexUpdateResult` - Response for update operation
   - `SearchIndexUpdaterStats` - Statistics about the updater

### PRD requirements met:
- Run an incremental update that adds/modifies markets rows ✓
- Detect which market rows changed (by hash of question+tags+description) ✓
- Regenerate embeddings only for changed/new markets ✓
- Verify BM25 and vector indices reflect the changes ✓

### Tests added (`tests/test_incremental_search_update.py`):
265 tests total (23 new tests for incremental search update feature).

**TestComputeMarketContentHash (7 tests):**
- `test_hash_with_all_fields`: Hash computation with all fields
- `test_hash_deterministic`: Deterministic hash for same input
- `test_hash_different_for_different_question`: Hash changes on question change
- `test_hash_different_for_different_tags`: Hash changes on tags change
- `test_hash_different_for_different_description`: Hash changes on description change
- `test_hash_with_none_fields`: Handles None fields
- `test_hash_tags_order_independent`: Tags sorted for consistent hash

**TestSearchIndexUpdater (9 tests):**
- `test_init_creates_hash_table`: Hash table creation on init
- `test_detect_new_markets`: Detection of new markets
- `test_detect_changed_markets`: Detection of changed markets
- `test_update_indices_full_rebuild`: Force rebuild functionality
- `test_update_indices_incremental_no_changes`: No-op when no changes
- `test_update_indices_incremental_with_changes`: Incremental update
- `test_update_specific_markets`: Update specific market IDs
- `test_update_specific_markets_empty_list`: Empty list handling
- `test_get_stats`: Statistics retrieval

**TestSearchIndexUpdaterWithNewMarkets (2 tests):**
- `test_detect_new_markets_after_add`: Detect newly added markets
- `test_incremental_update_adds_new_market`: New markets become searchable

**TestSearchIndexUpdaterAPI (4 tests):**
- `test_api_update_search_indices_force_rebuild`: Force rebuild endpoint
- `test_api_update_search_indices_incremental`: Incremental update endpoint
- `test_api_get_search_index_stats`: Stats endpoint
- `test_api_update_requires_bootstrap`: Bootstrap required check

**TestIncrementalSearchUpdateWithEvents (1 test):**
- `test_detect_change_from_tags_update`: Tag changes trigger index update

### PRD features marked as passing:
- Incremental updates: new/changed markets update the search indices without full rebuild

### Next steps for future sessions:
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
- Implement poly_data integration for live updates

---

## 2026-01-07: Markets Search API Endpoint with Unified Mode Support

### What was done:
Implemented the unified Markets Search API endpoint that supports multiple search modes (BM25, semantic, hybrid) with paginated ranked results and metadata needed for UI, including snippet generation with query term highlighting.

### Features implemented:
1. **Unified search endpoint** (`src/polymkt/api/main.py:383-538`)
   - `GET /api/markets/search` with mode parameter (bm25, semantic, hybrid)
   - Default mode is "hybrid" (falls back to bm25 without OpenAI API key)
   - Validates mode parameter and returns 400 for invalid modes
   - Graceful fallback from hybrid to BM25 when no OpenAI API key configured

2. **Unified response format** (`src/polymkt/models/schemas.py:164-178`)
   - `UnifiedMarketSearchResult` with all required UI fields:
     - id, question, tags, category, closed_time, event_id
     - relevance_score (interpretation depends on mode)
     - snippet (text with query terms highlighted)
     - bm25_score, semantic_score (for hybrid mode only)
   - `UnifiedSearchResponse` with results, count, total_count, has_more, mode

3. **Snippet generation with highlighting** (`src/polymkt/api/main.py:164-199`)
   - `generate_snippet()` helper function
   - Combines question and description for snippet text
   - Truncates to max_length (default 150 chars)
   - Case-insensitive highlighting of query terms with ** markers
   - Ignores single-character terms to avoid over-highlighting

4. **Search filtering and pagination**
   - Supports category filter
   - Supports closed_time_min and closed_time_max filters
   - Stable ordering by relevance_score and market_id for consistent pagination
   - Returns total_count and has_more for pagination UI

### PRD requirements met:
- Call GET /api/markets/search?q=election&mode=hybrid&limit=50&offset=0 ✓
- Response includes market_id, question, tags, category, closedTime, relevance_score, snippet ✓
- Pagination with offset works correctly ✓
- Stable ordering across pages for the same query ✓
- Supports filtering (category, closedTime range) in addition to text search ✓

### Tests added (`tests/test_unified_search_api.py`):
287 tests total (22 new tests for unified search API).

**TestUnifiedSearchModeParameter (4 tests):**
- `test_bm25_mode_returns_results`: BM25 mode works
- `test_default_mode_is_hybrid`: Default mode handling
- `test_invalid_mode_returns_400`: Invalid mode validation
- `test_semantic_mode_without_api_key_returns_400`: API key validation

**TestUnifiedSearchResponseFormat (3 tests):**
- `test_response_includes_all_required_fields`: All UI fields present
- `test_response_includes_snippet`: Snippet generation
- `test_relevance_score_is_numeric`: Score type validation

**TestUnifiedSearchPagination (3 tests):**
- `test_pagination_with_limit_and_offset`: Pagination works
- `test_has_more_flag_is_correct`: has_more flag accuracy
- `test_stable_ordering_for_pagination`: Deterministic ordering

**TestUnifiedSearchFilters (3 tests):**
- `test_filter_by_category`: Category filter
- `test_filter_by_closed_time_min`: Min date filter
- `test_filter_by_closed_time_max`: Max date filter

**TestSnippetGeneration (6 tests):**
- `test_generate_snippet_highlights_query_terms`: Basic highlighting
- `test_generate_snippet_case_insensitive_highlight`: Case preservation
- `test_generate_snippet_multiple_terms`: Multiple term highlighting
- `test_generate_snippet_truncates_long_text`: Truncation behavior
- `test_generate_snippet_includes_description_if_short_question`: Description inclusion
- `test_generate_snippet_ignores_single_char_terms`: Single char filtering

**TestUnifiedSearchEdgeCases (3 tests):**
- `test_empty_query_returns_empty_results`: Empty query handling
- `test_no_matches_returns_empty_results`: No matches handling
- `test_missing_parquet_returns_400`: Missing data error handling

### PRD features marked as passing:
- Markets Search API endpoint returns paginated ranked results with metadata needed for UI

### Next steps for future sessions:
- Implement incremental updates for events that refresh market tags in search indices
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
- Implement poly_data integration for live updates

---

## 2026-01-07: Event Change Detection for Search Index Updates

### What was done:
Implemented event change detection in the incremental search index updater, enabling detection of changed event tags and automatic updating of affected markets in the search indices.

### Features implemented:
1. **Event content hash tracking** (`src/polymkt/storage/search_index_updater.py`)
   - `compute_event_content_hash()` - Computes MD5 hash of event tags
   - `event_content_hashes` table stores hashes keyed by event_id
   - `_get_stored_event_hashes()` and `_compute_current_event_hashes()` methods
   - `detect_changed_events()` - Identifies new, changed, and deleted events

2. **Market-to-event linking**
   - `_get_markets_for_events()` - Finds markets linked to changed events via event_id
   - Deduplication ensures markets aren't updated twice (direct change + event change)
   - `event_affected_markets` stat tracks markets updated due to event changes only

3. **Updated `update_indices()` method**
   - Detects both market and event changes in a single pass
   - Finds markets affected by event tag changes
   - Combines all affected markets (deduplicated) for index update
   - Updates both market and event content hashes after processing
   - Returns comprehensive stats including event-related metrics

4. **API endpoint updates** (`src/polymkt/api/main.py`)
   - `POST /api/search-index/update` now returns event change statistics
   - `GET /api/search-index/stats` now returns event hash tracking statistics

5. **Schema updates** (`src/polymkt/models/schemas.py`)
   - `SearchIndexUpdateResult` - Added new_events, changed_events, deleted_events, event_affected_markets
   - `SearchIndexUpdaterStats` - Added total_event_hashes, event_first_updated, event_last_updated

### PRD requirements met:
- Run an incremental update that adds or modifies markets and events tags ✓
- Detect which events changed (by hash of tags) ✓
- Identify affected markets for embedding refresh via event linkage ✓
- Regenerate embeddings only for changed/new/affected markets ✓
- Verify BM25 and vector indices reflect the changes ✓

### Tests added (`tests/test_event_search_update.py`):
308 tests total (21 new tests for event change detection).

**TestComputeEventContentHash (6 tests):**
- `test_hash_with_tags`: Hash computation with tags
- `test_hash_deterministic`: Deterministic hash for same tags
- `test_hash_different_for_different_tags`: Different tags produce different hashes
- `test_hash_tags_order_independent`: Tags sorted for consistent hash
- `test_hash_with_none_tags`: Handles None tags
- `test_hash_with_empty_tags`: Empty list equals None

**TestEventChangeDetection (3 tests):**
- `test_detect_new_events`: Detection of new events
- `test_no_changes_after_initial_update`: No changes after first build
- `test_check_events_available_with_events`: Events availability check

**TestGetMarketsForEvents (4 tests):**
- `test_get_markets_for_single_event`: Single event to markets lookup
- `test_get_markets_for_multiple_events`: Multiple events to markets lookup
- `test_get_markets_for_empty_events`: Empty list handling
- `test_get_markets_for_nonexistent_event`: Nonexistent event handling

**TestUpdateIndicesWithEvents (4 tests):**
- `test_force_rebuild_stores_event_hashes`: Force rebuild stores hashes
- `test_incremental_update_detects_event_changes`: Event detection works
- `test_incremental_update_includes_event_affected_markets`: Affected markets included
- `test_no_changes_returns_correct_result`: No changes scenario

**TestSearchIndexUpdaterStats (2 tests):**
- `test_stats_includes_event_hashes`: Stats include event hashes
- `test_stats_empty_when_no_index`: Empty stats before build

**TestAPIEventSearchUpdate (2 tests):**
- `test_api_update_returns_event_fields`: API returns event fields
- `test_api_stats_returns_event_hash_fields`: API stats include event fields

### PRD features marked as passing:
- Incremental updates: new/changed markets and events update the search indices without full rebuild

### Next steps for future sessions:
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
- Implement poly_data integration for live updates
- Implement API endpoints for dataset CRUD and backtest execution

---

## 2026-01-07: Datasets Persistence Feature

### What was done:
Implemented the Datasets persistence feature that allows users to save, list, update, and delete market sets with filters and market lists for reuse in backtesting and analysis.

### Features implemented:
1. **Dataset schema models** (`src/polymkt/models/schemas.py:271-340`)
   - `DatasetFilters` - Filters used to create a dataset (query, category, tags, closed_time range, min_volume)
   - `DatasetSchema` - Full dataset with id, name, description, filters, market_ids, excluded_market_ids, timestamps
   - `DatasetCreateRequest` - Request to create a new dataset with validation
   - `DatasetUpdateRequest` - Request for partial updates
   - `DatasetSummary` - Summary for list views with market_count
   - `DatasetListResponse` - Paginated list response with count, total_count, has_more

2. **DatasetStore class** (`src/polymkt/storage/datasets.py`)
   - SQLite-backed storage for datasets in the metadata database
   - `create_dataset()` - Creates a new dataset with generated UUID and timestamps
   - `get_dataset()` - Retrieves a dataset by ID, raises DatasetNotFoundError if not found
   - `update_dataset()` - Partial update of dataset fields
   - `delete_dataset()` - Removes a dataset
   - `list_datasets()` - Paginated listing ordered by updated_at DESC
   - Proper JSON serialization of filters (including datetime fields)
   - Proper JSON serialization of market_ids and excluded_market_ids lists

3. **API endpoints** (`src/polymkt/api/main.py:1096-1228`)
   - `POST /api/datasets` - Create a new dataset (returns 201)
   - `GET /api/datasets` - List datasets with pagination (limit, offset)
   - `GET /api/datasets/{dataset_id}` - Get a dataset by ID
   - `PUT /api/datasets/{dataset_id}` - Update a dataset (partial update supported)
   - `DELETE /api/datasets/{dataset_id}` - Delete a dataset

### PRD requirements met:
- Create a dataset by filters or by explicit market list ✓
- Save dataset to SQLite (id, name, description, filters, included market_ids) ✓
- List datasets and open one ✓
- Edit dataset (exclude/include some markets) and re-save ✓

### Tests added (`tests/test_datasets.py`):
348 tests total (40 new tests for datasets feature).

**TestDatasetStoreCreation (2 tests):**
- `test_init_creates_table`: Table creation verification
- `test_init_creates_parent_dirs`: Directory creation

**TestDatasetCreate (4 tests):**
- `test_create_dataset_basic`: Basic dataset creation
- `test_create_dataset_with_filters`: Filters with query, category, tags
- `test_create_dataset_with_excluded_markets`: Excluded markets list
- `test_create_multiple_datasets`: Multiple datasets

**TestDatasetGet (3 tests):**
- `test_get_dataset_by_id`: Get by ID
- `test_get_nonexistent_dataset_raises`: 404 handling
- `test_get_dataset_with_filters`: Filters with datetime fields

**TestDatasetUpdate (6 tests):**
- `test_update_dataset_name`: Update name only
- `test_update_dataset_market_ids`: Update market list
- `test_update_dataset_excluded_markets`: Update exclusions
- `test_update_dataset_filters`: Update filters
- `test_update_nonexistent_dataset_raises`: 404 handling
- `test_update_preserves_created_at`: Timestamp preservation

**TestDatasetDelete (3 tests):**
- `test_delete_dataset`: Delete functionality
- `test_delete_nonexistent_dataset_raises`: 404 handling
- `test_delete_updates_count`: Count verification

**TestDatasetList (5 tests):**
- `test_list_empty`: Empty list
- `test_list_returns_summaries`: Summary format
- `test_list_pagination_limit`: Limit parameter
- `test_list_pagination_offset`: Offset parameter
- `test_list_ordered_by_updated_at`: Ordering

**TestDatasetsAPICreate (5 tests):**
- API endpoint tests for create with validation

**TestDatasetsAPIList (3 tests):**
- API endpoint tests for list with pagination

**TestDatasetsAPIGet (2 tests):**
- API endpoint tests for get by ID

**TestDatasetsAPIUpdate (3 tests):**
- API endpoint tests for update (full and partial)

**TestDatasetsAPIDelete (2 tests):**
- API endpoint tests for delete

**TestDatasetsIntegration (2 tests):**
- `test_full_crud_workflow`: Complete CRUD workflow test
- `test_multiple_datasets_workflow`: Multiple datasets management

### PRD features marked as passing:
- Persist 'Datasets' (market sets + filters + market list) so users can reuse them in the UI

### Next steps for future sessions:
- Implement backtests persistence (save, list, rerun)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)
