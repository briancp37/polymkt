## 2026-01-06: Bootstrap Import Feature Implementation

### What was done:
Implemented the foundational bootstrap import feature which reads existing poly_data CSV files and converts them to a professional DuckDB-over-Parquet analytics stack.

### Features implemented:
1. **Bootstrap CSV to Parquet pipeline** (`src/polymkt/pipeline/bootstrap.py`)
   - Reads markets.csv, trades.csv, orderFilled.csv from data/ directory
   - Converts to Parquet with ZSTD compression
   - Proper schema with typed columns (timestamps as UTC datetime, numerics as float64)
   - Column renaming from camelCase to snake_case

2. **DuckDB view layer** (`src/polymkt/storage/duckdb_layer.py`)
   - Creates v_markets, v_trades, v_order_filled, v_trades_with_markets views
   - v_trades_with_markets includes days_to_exp derived field
   - Query interface with market_id and time range filters

3. **Metadata store** (`src/polymkt/storage/metadata.py`)
   - SQLite-backed run history tracking
   - Stores run_id, start/end time, rows read/written, schema version
   - Watermark persistence for incremental updates

4. **FastAPI endpoints** (`src/polymkt/api/main.py`)
   - POST /api/bootstrap - Run bootstrap import
   - GET /api/runs - List pipeline runs
   - GET /api/runs/{run_id} - Get run details
   - POST /api/query/trades - Query trades with filters
   - GET /api/watermarks - Get current watermarks
   - GET /health - Health check

### PRD features marked as passing:
- Bootstrap from existing backfilled poly_data CSVs
- Convert backfilled trades.csv into compressed Parquet
- Create DuckDB database layer with views over Parquet
- Maintain metadata store for run history and watermarks

### Tech stack:
- Python 3.11+
- FastAPI for API
- DuckDB for analytics queries
- PyArrow for Parquet operations
- Pydantic for data validation
- SQLite for metadata storage

### Tests:
12 tests passing covering bootstrap, API, and query functionality.
Type checking passes with mypy.

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags)
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)

---

## 2026-01-06: Field Normalization and Validation Feature

### What was done:
Implemented comprehensive field normalization and validation for the bootstrap import pipeline, ensuring data quality and consistency.

### Features implemented:
1. **Normalization module** (`src/polymkt/pipeline/normalize.py`)
   - `normalize_address()` - Normalizes Ethereum addresses to lowercase with 0x prefix
   - `normalize_timestamp()` - Normalizes timestamps to UTC datetime, supporting multiple formats
   - `normalize_numeric()` - Validates numeric fields with optional min/max bounds
   - `ValidationResult` dataclass to track valid rows vs quarantined rows

2. **Entity-specific validation functions**
   - `validate_and_normalize_trades()` - Validates trades with price bounds (0-1), required fields, address normalization
   - `validate_and_normalize_markets()` - Validates markets with required id/question fields
   - `validate_and_normalize_order_filled()` - Validates order events with required timestamp/hash

3. **Bootstrap pipeline integration**
   - Added `validate_data` and `normalize_addresses` parameters to `run_bootstrap()`
   - Invalid rows are quarantined (not written to Parquet) with clear error logs
   - `BootstrapSummary` now includes `rows_quarantined` counts per entity
   - Structured logging for validation errors with sample messages

4. **Edge case handling**
   - Bad timestamps: Invalid formats are rejected with clear error messages
   - Missing required fields: Rows quarantined with field-specific errors
   - Invalid prices: Values outside 0-1 range are quarantined
   - Invalid addresses: Non-hex or wrong-length addresses normalized to None
   - NaN/Infinity values: Rejected with clear logs

### PRD features marked as passing:
- Normalize and type-cast fields consistently (timestamps, numerics, addresses, directions)

### Tests:
50 tests passing (38 new tests for normalization module).
- Unit tests for `normalize_address`, `normalize_timestamp`, `normalize_numeric`
- Integration tests for entity validation functions
- End-to-end test with bootstrap pipeline and edge cases

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Query interface enhancements (stable ordering option)

---

## 2026-01-06: Query Interface Enhancement Feature

### What was done:
Implemented the complete query interface for filtering trades by market_id and time range, with proper pagination support and stable ordering options.

### Features implemented:
1. **Enhanced `query_trades()` method** (`src/polymkt/storage/duckdb_layer.py:142-238`)
   - Added `order_by` parameter supporting multiple columns (e.g., "timestamp,transaction_hash" for stable ordering)
   - Added `order_dir` parameter ("ASC" or "DESC")
   - Returns tuple of (trades, total_count) for proper pagination UI
   - Input validation for order_by columns (prevents SQL injection)
   - Allowed columns: timestamp, price, usd_amount, token_amount, market_id, transaction_hash

2. **Enhanced API response** (`src/polymkt/api/main.py`)
   - `TradesQueryRequest` now includes `order_by` and `order_dir` parameters
   - `TradesQueryResponse` now includes:
     - `count`: Number of rows in current page
     - `total_count`: Total matching rows (for "Results 1-50 of 1,234" display)
     - `has_more`: Boolean indicating if more pages exist
   - Proper error handling for invalid order parameters (returns 400)

3. **PRD requirements met:**
   - Filter by single market_id ✓
   - Filter by start/end timestamps ✓
   - Results sorted with stable ordering option (timestamp,transaction_hash) ✓
   - Pagination with limit/offset ✓
   - Response includes total_count for pagination UI ✓

### Tests:
64 tests passing (14 new tests for query interface).
- `TestQueryFiltering`: Tests for market_id and time range filtering
- `TestQueryOrdering`: Tests for ordering by different columns, ASC/DESC, composite keys
- `TestQueryPagination`: Tests for limit, offset, total_count, has_more
- `TestQueryIntegration`: End-to-end test combining all features

### PRD features marked as passing:
- Query interface: filter by a single market_id and time range

---

## 2026-01-06: Query Interface for 100+ Market IDs Feature

### What was done:
Implemented efficient querying for multiple market_ids (100+), enabling research workflows that need to analyze trades across many markets simultaneously.

### Features implemented:
1. **Multiple market_ids query support** (`src/polymkt/storage/duckdb_layer.py:142-238`)
   - `market_ids` parameter accepts a list of market IDs
   - Uses SQL `IN` clause with parameterized queries (safe from SQL injection)
   - `market_id` (single) takes precedence over `market_ids` (multiple) if both provided
   - Combined with time range filters and pagination

2. **API endpoint support** (`src/polymkt/api/main.py`)
   - `TradesQueryRequest.market_ids: list[str] | None` parameter added
   - Works with existing pagination (limit/offset) and ordering
   - Response size controls enforced via limit parameter

3. **DuckDB predicate pushdown verification**
   - Query plan analysis shows filter operations are applied efficiently
   - DuckDB optimizes `IN` clause filtering against Parquet files

### Tests added (`tests/test_query_interface.py`, `tests/test_api.py`):
78 tests passing (14 new tests for multiple market_ids).

**TestQueryMultipleMarketIds (6 tests):**
- `test_filter_by_two_market_ids`: Basic multi-market query
- `test_filter_by_subset_market_ids`: Verify only requested markets returned
- `test_filter_by_nonexistent_market_ids`: Empty result handling
- `test_multiple_market_ids_with_time_range`: Combined filters
- `test_multiple_market_ids_with_pagination`: Pagination with multi-market
- `test_single_market_id_parameter_takes_precedence`: Precedence rule

**TestQuery100PlusMarketIds (5 tests):**
- `test_query_100_plus_market_ids`: 110 market_ids query (330 trades)
- `test_query_100_plus_market_ids_with_time_filter`: Combined time+market filter
- `test_query_100_plus_market_ids_pagination`: Pagination with 100 markets
- `test_query_150_market_ids`: Full dataset query (450 trades)
- `test_query_plan_uses_filter_pushdown`: EXPLAIN ANALYZE verification

**TestQueryTradesAPI (3 tests):**
- `test_query_multiple_market_ids`: API endpoint test
- `test_query_multiple_market_ids_with_pagination`: API pagination test
- `test_query_single_market_id_precedence`: API precedence test

### PRD features marked as passing:
- Query interface: filter by many market_ids (100+), efficiently

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Add derived field days_to_exp

---

## 2026-01-06: days_to_exp Derived Field Feature

### What was done:
Implemented the `days_to_exp` derived field for trades, enabling backtesting strategies that rely on days-to-expiry filtering (e.g., "buy at 90 days to expiry").

### Features implemented:
1. **`query_trades_with_markets()` method** (`src/polymkt/storage/duckdb_layer.py:240-353`)
   - Queries from `v_trades_with_markets` view which joins trades to markets
   - Includes `days_to_exp` derived field: `(closed_time - timestamp) / 86400.0` in days
   - Supports filtering by `days_to_exp_min` and `days_to_exp_max` parameters
   - Supports ordering by `days_to_exp` and other market columns (question, category, closed_time)
   - Full pagination support (limit/offset with total_count)

2. **API endpoint** (`src/polymkt/api/main.py:167-222`)
   - `POST /api/query/trades_with_markets` endpoint
   - Request accepts `days_to_exp_min` and `days_to_exp_max` float parameters
   - Combines with existing filters (market_id, market_ids, time range)
   - Returns trades with market data (question, category, closed_time) plus days_to_exp

3. **days_to_exp formula** (documented in view definition)
   - Computed as: `EXTRACT(EPOCH FROM (closed_time - timestamp)) / 86400.0`
   - Returns NULL when market has no closed_time
   - Filtering by range excludes NULL values (markets without expiry)

### PRD requirements met:
- Join trades to markets by market_id ✓
- Compute days_to_exp = (closedTime - trade_timestamp) in days ✓
- Verify days_to_exp is correct for known sample timestamps ✓ (comprehensive tests)
- Verify days_to_exp is persisted/computed in a view ✓ (v_trades_with_markets)
- Verify days_to_exp can be filtered efficiently (e.g., between 89 and 91) ✓

### Tests added (`tests/test_days_to_exp.py`, `tests/test_api.py`):
101 tests passing (23 new tests for days_to_exp feature).

**TestDaysToExpCorrectness (5 tests):**
- `test_days_to_exp_formula`: Verifies ~90 day calculation
- `test_days_to_exp_180_days`: Verifies ~180 day calculation
- `test_days_to_exp_30_days`: Verifies ~30 day calculation
- `test_days_to_exp_null_when_no_closed_time`: NULL handling
- `test_days_to_exp_includes_market_columns`: Joined columns present

**TestDaysToExpFiltering (6 tests):**
- `test_filter_by_days_to_exp_min`: Minimum filter
- `test_filter_by_days_to_exp_max`: Maximum filter
- `test_filter_by_days_to_exp_range_89_to_91`: PRD requirement
- `test_filter_days_to_exp_combined_with_market_id`: Combined filters
- `test_filter_days_to_exp_combined_with_time_range`: Combined filters
- `test_filter_excludes_null_days_to_exp`: NULL exclusion

**TestDaysToExpOrdering (2 tests):**
- `test_order_by_days_to_exp_asc`: Ascending order
- `test_order_by_days_to_exp_desc`: Descending order

**TestDaysToExpPagination (2 tests):**
- `test_pagination_with_days_to_exp_filter`: Pagination works
- `test_total_count_with_days_to_exp_filter`: Count correct

**TestDaysToExpValidation (2 tests):**
- `test_invalid_order_by_column_raises_error`: Validation
- `test_extended_order_by_columns_available`: New columns work

**TestQueryTradesWithMarketsAPI (6 tests):**
- API endpoint tests for days_to_exp filtering and ordering

### PRD features marked as passing:
- Add derived field days_to_exp using markets.closedTime as expiry

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Parquet Partitioning Feature

### What was done:
Implemented Parquet partitioning for trades data using year/month/day + market_id hash bucket strategy, enabling efficient query filtering and partition pruning.

### Features implemented:
1. **Partitioning infrastructure** (`src/polymkt/storage/parquet.py`)
   - `compute_hash_bucket()` - Deterministic MD5-based hash bucketing for market_id
   - `add_partition_columns()` - Extracts year, month, day from timestamp and computes hash_bucket
   - `ParquetWriter` updated with `partitioning_enabled` and `hash_bucket_count` parameters
   - Hive-style partitioned writes: `trades/year=YYYY/month=MM/day=DD/hash_bucket=N/*.parquet`

2. **DuckDB layer updates** (`src/polymkt/storage/duckdb_layer.py`)
   - `get_view_definitions()` function dynamically generates views based on partitioning mode
   - Partitioned mode uses `read_parquet('{path}/trades/**/*.parquet', hive_partitioning=true)`
   - `DuckDBLayer` constructor accepts `partitioned` parameter
   - Added `explain_query()` method for query plan inspection

3. **Configuration** (`src/polymkt/config.py`)
   - `parquet_partitioning_enabled: bool = False` (opt-in for backward compatibility)
   - `parquet_hash_bucket_count: int = 8` (configurable bucket count)

4. **Bootstrap integration** (`src/polymkt/pipeline/bootstrap.py`)
   - `run_bootstrap()` accepts `partitioning_enabled` and `hash_bucket_count` parameters
   - Passes partitioning config to `ParquetWriter` and `DuckDBLayer`

### PRD requirements met:
- Configure partitioning strategy (year/month/day + market_id hash bucket) ✓
- Run Parquet write with partitioning enabled ✓
- Run DuckDB query filtering by market_id and time range ✓
- Verify partition pruning via query plan inspection ✓
- Verify query returns correct rows ✓

### Tests added (`tests/test_partitioning.py`):
116 tests passing (15 new tests for partitioning feature).

**TestHashBucketComputation (3 tests):**
- `test_compute_hash_bucket_returns_valid_range`: Buckets in [0, bucket_count)
- `test_compute_hash_bucket_deterministic`: Same market_id → same bucket
- `test_compute_hash_bucket_distribution`: Buckets distribute across IDs

**TestAddPartitionColumns (1 test):**
- `test_add_partition_columns_extracts_date_parts`: Year/month/day extraction

**TestPartitionedParquetWriter (2 tests):**
- `test_write_trades_partitioned_creates_directory_structure`: Hive directory layout
- `test_write_trades_monolithic_creates_single_file`: Backward compatible

**TestBootstrapWithPartitioning (2 tests):**
- `test_bootstrap_with_partitioning_creates_partitioned_trades`: Partitioned output
- `test_bootstrap_without_partitioning_creates_single_file`: Default behavior

**TestDuckDBPartitionedReads (3 tests):**
- `test_duckdb_can_read_partitioned_trades`: Views work with partitioned data
- `test_duckdb_query_partitioned_by_market_id`: Market filter works
- `test_duckdb_query_partitioned_by_time_range`: Time filter works

**TestPartitionPruning (3 tests):**
- `test_query_plan_shows_filter_for_partitioned_data`: EXPLAIN works
- `test_query_plan_with_time_filter`: Time filter in plan
- `test_query_returns_correct_rows_after_partitioning`: Data integrity

**TestBackwardCompatibility (1 test):**
- `test_existing_tests_work_with_partitioning_disabled`: No regressions

### PRD features marked as passing:
- Partition Parquet to speed up common filters (time + market_id) using a professional DuckDB-over-Parquet setup

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Raw/Analytics Layer Separation Feature

### What was done:
Implemented immutable raw layer and curated analytics layer separation for trades data, enabling professional data pipeline patterns where raw data is preserved and derived fields are computed in a separate analytics layer.

### Features implemented:
1. **Configuration updates** (`src/polymkt/config.py`)
   - Added `parquet_raw_dir` (data/parquet/raw) for immutable source data
   - Added `parquet_analytics_dir` (data/parquet/analytics) for derived analytics data

2. **Parquet analytics schema** (`src/polymkt/storage/parquet.py`)
   - Added `TRADES_ANALYTICS_SCHEMA` with `days_to_exp` derived field
   - Added `write_trades_analytics()` method supporting partitioned/monolithic writes
   - Supports same partitioning strategy (year/month/day/hash_bucket) as raw layer

3. **Curate pipeline** (`src/polymkt/pipeline/curate.py`)
   - `run_curate()` function reads from raw layer and builds analytics layer
   - Uses DuckDB in-memory join to compute `days_to_exp` from trades + markets
   - Raw layer is NEVER modified (verified by tests)
   - Returns `CurateSummary` with run metadata (rows read/written, files created)

4. **DuckDB layered views** (`src/polymkt/storage/duckdb_layer.py`)
   - Added `get_layered_view_definitions()` for raw/analytics layer mode
   - Raw views: `v_markets_raw`, `v_trades_raw`, `v_order_filled_raw`
   - Analytics views: `v_trades_analytics` (with materialized days_to_exp)
   - Combined view: `v_trades_with_markets` (analytics trades + market metadata)
   - Backward compatibility: `v_markets`, `v_trades`, `v_order_filled` alias to raw views
   - `DuckDBLayer` supports `layered=True` mode with `raw_dir` and `analytics_dir` params

5. **API endpoint** (`src/polymkt/api/main.py`)
   - `POST /api/curate` endpoint to run curate step via API

6. **Schema models** (`src/polymkt/models/schemas.py`)
   - Added `CurateSummary` Pydantic model for curate run results

### PRD requirements met:
- Ingest/convert CSV into a raw Parquet layer ✓
- Run a curate step that builds an analytics Parquet layer ✓
- Verify raw layer is unchanged after curate step ✓ (MD5 hash verification tests)
- Verify analytics layer contains derived fields (days_to_exp) ✓
- Verify both layers can be queried independently ✓

### Tests added (`tests/test_layers.py`):
129 tests passing (13 new tests for layer separation).

**TestRawLayerImmutability (2 tests):**
- `test_raw_layer_unchanged_after_curate`: MD5 hash verification
- `test_raw_layer_row_count_preserved`: Row count preservation

**TestCurateStep (2 tests):**
- `test_curate_creates_analytics_parquet`: Analytics file creation
- `test_curate_summary_has_correct_counts`: Summary accuracy

**TestAnalyticsLayerDerivedFields (3 tests):**
- `test_analytics_layer_has_days_to_exp`: Schema verification
- `test_analytics_days_to_exp_computed_correctly`: ~90 day calculation
- `test_analytics_days_to_exp_null_for_no_closed_time`: NULL handling

**TestLayeredDuckDBViews (4 tests):**
- `test_layered_views_created`: All 8 views created
- `test_raw_views_have_no_derived_fields`: Raw view schema
- `test_analytics_view_has_derived_fields`: Analytics view schema
- `test_both_layers_queryable_independently`: Independent queries

**TestBackwardCompatibility (2 tests):**
- `test_legacy_mode_still_works`: Non-layered mode
- `test_alias_views_work_in_layered_mode`: Alias views

### PRD features marked as passing:
- Maintain an immutable raw layer and a curated analytics layer for trades

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Incremental Updates with Watermark-Based Fetching Feature

### What was done:
Implemented the incremental update pipeline using watermark-based filtering to fetch only new data since the last update, with deduplication to ensure idempotent updates.

### Features implemented:
1. **Incremental update pipeline** (`src/polymkt/pipeline/update.py`)
   - Reads current watermarks from metadata store for each entity
   - Filters incoming data to only rows after the watermark timestamp
   - Supports watermark-based filtering for trades, markets, and order_filled
   - Runtime is proportional to new data, not total history

2. **Deduplication logic**
   - Uses `transaction_hash` as dedupe key for trades and order_filled
   - Uses `id` for markets (with upsert support for updates)
   - Efficiently skips already-ingested data via set lookups
   - Logs skipped/deduplicated rows for visibility

3. **Upsert/append strategy for Parquet**
   - Trades/order_filled: Append-only with transaction_hash deduplication
   - Markets: Full upsert using DuckDB (updates replace existing, new rows added)
   - Preserves data integrity across repeated update runs

4. **API endpoint** (`src/polymkt/api/main.py`)
   - `POST /api/update` endpoint for incremental updates
   - Returns `UpdateSummary` with rows_read, rows_written, rows_skipped, rows_updated
   - Includes watermark_before and watermark_after for audit trail

5. **UpdateSummary schema** (`src/polymkt/models/schemas.py`)
   - Pydantic model for update operation results
   - Tracks all row counts and watermark state changes

### PRD requirements met:
- Read current watermark from local state ✓
- Run update to fetch only new data since watermark ✓
- Append new rows to Parquet ✓
- Update DuckDB views if needed ✓
- Verify watermark advances and is persisted ✓
- Verify runtime is proportional to new data ✓
- Run incremental update twice without duplicating data ✓
- Verify dedupe key logic is enforced (transaction_hash) ✓
- Verify logs explicitly report dedupe/skips ✓

### Tests added (`tests/test_update.py`):
140 tests passing (11 new tests for incremental updates).

**TestIncrementalUpdate (4 tests):**
- `test_update_with_new_trades`: Verifies new trades are appended
- `test_update_with_new_markets`: Verifies new markets are added
- `test_update_creates_run_record`: Verifies run tracking
- `test_update_advances_watermark`: Verifies watermark progression

**TestIdempotentUpdates (3 tests):**
- `test_repeated_update_does_not_duplicate_trades`: Idempotency verification
- `test_transaction_hash_uniqueness_enforced`: Dedupe key enforcement
- `test_dedupe_logs_skipped_rows`: Skip logging verification

**TestUpdateRuntimeProportionality (1 test):**
- `test_update_runtime_scales_with_new_data`: Runtime scaling verification

**TestWatermarkFiltering (1 test):**
- `test_watermark_filters_old_data`: Watermark filter verification

**TestDuckDBViewsRefresh (1 test):**
- `test_duckdb_views_reflect_new_data`: View refresh verification

**TestMarketsUpsert (1 test):**
- `test_existing_market_updated`: Market upsert verification

### PRD features marked as passing:
- Incremental updates use poly_data-like logic (fast forward from last watermark)
- Idempotent update runs (re-running the same update does not duplicate trades)

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-06: Events Bootstrap and Schema Validation Feature

### What was done:
Implemented the events bootstrap pipeline and schema validation feature that loads events with tags from CSV and joins them to markets, preserving custom columns (category, closedTime) and deriving markets.tags from events.

### Features implemented:
1. **Events CSV processing** (`src/polymkt/pipeline/bootstrap.py:120-182`)
   - `_read_events_csv()` parses events CSV with JSON-encoded tags list
   - Handles various tag formats: JSON array, comma-separated, single value
   - Creates events Parquet with event_id, tags (list<string>), title, description, created_at

2. **Events-to-markets tag join** (`src/polymkt/pipeline/bootstrap.py:184-277`)
   - `_join_events_tags_to_markets()` uses DuckDB in-memory join
   - Left join ensures markets without events get empty tags []
   - Logs warning for markets with event_id but no matching event

3. **Schema validation with actionable errors** (`src/polymkt/pipeline/bootstrap.py:279-362`)
   - `SchemaValidationError` exception with missing_fields and invalid_fields details
   - `validate_schema_requirements()` validates:
     - Required markets fields (id, question)
     - Required events fields if provided (event_id, tags)
     - Join key (event_id) in markets when events are required
   - Fails fast with clear error messages for remediation

4. **Bootstrap integration** (`src/polymkt/pipeline/bootstrap.py:364-636`)
   - Added `events_csv` and `require_events_for_tags` parameters
   - Events processed before markets to enable tag join
   - Events validation, normalization, and Parquet write
   - Schema validation before market-events join
   - Preserves category and closedTime on markets

5. **DuckDB views for events** (`src/polymkt/storage/duckdb_layer.py:29-118`)
   - Added `v_events` view with event_id, tags, title, description, created_at
   - Updated `v_markets` to include event_id and tags columns
   - Updated `v_trades_with_markets` to include tags
   - Optional events view creation (gracefully skips if no events.parquet)

6. **Parquet events schema** (`src/polymkt/storage/parquet.py:15-21`)
   - `EVENTS_SCHEMA` with tags as list<string> type
   - `write_events()` method in `ParquetWriter`

7. **BootstrapSummary updated** (`src/polymkt/models/schemas.py:95-109`)
   - Added `events_rows` field for event row count

### PRD requirements met:
- Load data/markets.csv with category and closedTime present ✓
- Run events bootstrap that loads event_id and tags into events table ✓
- Join events.tags onto markets via event_id to produce markets.tags ✓
- Run bootstrap import into storage layer ✓
- Verify updated market rows contain category, closedTime, and derived tags ✓
- Verify schema validation fails fast with actionable errors ✓

### Tests added (`tests/test_events_schema.py`):
160 tests passing (20 new tests for events/schema feature).

**TestReadEventsCSV (3 tests):**
- `test_read_events_csv_parses_tags`: Verifies events CSV is read with tags parsed
- `test_read_events_csv_tags_are_lists`: Verifies tags are list of strings
- `test_read_events_csv_empty_tags`: Verifies empty tags handled as []

**TestJoinEventsToMarkets (2 tests):**
- `test_join_events_tags_to_markets`: Verifies tags joined correctly
- `test_join_events_to_markets_no_match`: Verifies unmatched markets get []

**TestSchemaValidation (6 tests):**
- `test_validate_schema_requirements_passes`: Valid schema passes
- `test_validate_schema_requires_market_id`: Missing id fails
- `test_validate_schema_requires_market_question`: Missing question fails
- `test_validate_schema_requires_event_id_for_tags_join`: Missing event_id fails when required
- `test_validate_schema_requires_events_table_for_tags`: Missing events table fails when required
- `test_validate_schema_requires_events_tags_column`: Missing tags column fails

**TestBootstrapWithEvents (6 tests):**
- `test_bootstrap_with_events_creates_events_parquet`: Events Parquet created
- `test_bootstrap_with_events_joins_tags_to_markets`: Tags joined to markets
- `test_bootstrap_preserves_category_and_closed_time`: Custom columns preserved
- `test_bootstrap_sets_events_watermark`: Watermark set for events
- `test_bootstrap_without_events_works`: Backward compatible without events
- `test_bootstrap_fails_when_events_required_but_missing`: Fails when required but missing

**TestDuckDBEventsView (3 tests):**
- `test_duckdb_creates_events_view`: v_events view created
- `test_duckdb_markets_view_includes_tags`: v_markets includes tags
- `test_duckdb_trades_with_markets_includes_tags`: v_trades_with_markets includes tags

### PRD features marked as passing:
- Schema validation includes custom markets columns (category, closedTime) and tags derived from events

### Next steps for future sessions:
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
