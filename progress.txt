## 2026-01-06: Bootstrap Import Feature Implementation

### What was done:
Implemented the foundational bootstrap import feature which reads existing poly_data CSV files and converts them to a professional DuckDB-over-Parquet analytics stack.

### Features implemented:
1. **Bootstrap CSV to Parquet pipeline** (`src/polymkt/pipeline/bootstrap.py`)
   - Reads markets.csv, trades.csv, orderFilled.csv from data/ directory
   - Converts to Parquet with ZSTD compression
   - Proper schema with typed columns (timestamps as UTC datetime, numerics as float64)
   - Column renaming from camelCase to snake_case

2. **DuckDB view layer** (`src/polymkt/storage/duckdb_layer.py`)
   - Creates v_markets, v_trades, v_order_filled, v_trades_with_markets views
   - v_trades_with_markets includes days_to_exp derived field
   - Query interface with market_id and time range filters

3. **Metadata store** (`src/polymkt/storage/metadata.py`)
   - SQLite-backed run history tracking
   - Stores run_id, start/end time, rows read/written, schema version
   - Watermark persistence for incremental updates

4. **FastAPI endpoints** (`src/polymkt/api/main.py`)
   - POST /api/bootstrap - Run bootstrap import
   - GET /api/runs - List pipeline runs
   - GET /api/runs/{run_id} - Get run details
   - POST /api/query/trades - Query trades with filters
   - GET /api/watermarks - Get current watermarks
   - GET /health - Health check

### PRD features marked as passing:
- Bootstrap from existing backfilled poly_data CSVs
- Convert backfilled trades.csv into compressed Parquet
- Create DuckDB database layer with views over Parquet
- Maintain metadata store for run history and watermarks

### Tech stack:
- Python 3.11+
- FastAPI for API
- DuckDB for analytics queries
- PyArrow for Parquet operations
- Pydantic for data validation
- SQLite for metadata storage

### Tests:
12 tests passing covering bootstrap, API, and query functionality.
Type checking passes with mypy.

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags)
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)

---

## 2026-01-06: Field Normalization and Validation Feature

### What was done:
Implemented comprehensive field normalization and validation for the bootstrap import pipeline, ensuring data quality and consistency.

### Features implemented:
1. **Normalization module** (`src/polymkt/pipeline/normalize.py`)
   - `normalize_address()` - Normalizes Ethereum addresses to lowercase with 0x prefix
   - `normalize_timestamp()` - Normalizes timestamps to UTC datetime, supporting multiple formats
   - `normalize_numeric()` - Validates numeric fields with optional min/max bounds
   - `ValidationResult` dataclass to track valid rows vs quarantined rows

2. **Entity-specific validation functions**
   - `validate_and_normalize_trades()` - Validates trades with price bounds (0-1), required fields, address normalization
   - `validate_and_normalize_markets()` - Validates markets with required id/question fields
   - `validate_and_normalize_order_filled()` - Validates order events with required timestamp/hash

3. **Bootstrap pipeline integration**
   - Added `validate_data` and `normalize_addresses` parameters to `run_bootstrap()`
   - Invalid rows are quarantined (not written to Parquet) with clear error logs
   - `BootstrapSummary` now includes `rows_quarantined` counts per entity
   - Structured logging for validation errors with sample messages

4. **Edge case handling**
   - Bad timestamps: Invalid formats are rejected with clear error messages
   - Missing required fields: Rows quarantined with field-specific errors
   - Invalid prices: Values outside 0-1 range are quarantined
   - Invalid addresses: Non-hex or wrong-length addresses normalized to None
   - NaN/Infinity values: Rejected with clear logs

### PRD features marked as passing:
- Normalize and type-cast fields consistently (timestamps, numerics, addresses, directions)

### Tests:
50 tests passing (38 new tests for normalization module).
- Unit tests for `normalize_address`, `normalize_timestamp`, `normalize_numeric`
- Integration tests for entity validation functions
- End-to-end test with bootstrap pipeline and edge cases

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Query interface enhancements (stable ordering option)

---

## 2026-01-06: Query Interface Enhancement Feature

### What was done:
Implemented the complete query interface for filtering trades by market_id and time range, with proper pagination support and stable ordering options.

### Features implemented:
1. **Enhanced `query_trades()` method** (`src/polymkt/storage/duckdb_layer.py:142-238`)
   - Added `order_by` parameter supporting multiple columns (e.g., "timestamp,transaction_hash" for stable ordering)
   - Added `order_dir` parameter ("ASC" or "DESC")
   - Returns tuple of (trades, total_count) for proper pagination UI
   - Input validation for order_by columns (prevents SQL injection)
   - Allowed columns: timestamp, price, usd_amount, token_amount, market_id, transaction_hash

2. **Enhanced API response** (`src/polymkt/api/main.py`)
   - `TradesQueryRequest` now includes `order_by` and `order_dir` parameters
   - `TradesQueryResponse` now includes:
     - `count`: Number of rows in current page
     - `total_count`: Total matching rows (for "Results 1-50 of 1,234" display)
     - `has_more`: Boolean indicating if more pages exist
   - Proper error handling for invalid order parameters (returns 400)

3. **PRD requirements met:**
   - Filter by single market_id ✓
   - Filter by start/end timestamps ✓
   - Results sorted with stable ordering option (timestamp,transaction_hash) ✓
   - Pagination with limit/offset ✓
   - Response includes total_count for pagination UI ✓

### Tests:
64 tests passing (14 new tests for query interface).
- `TestQueryFiltering`: Tests for market_id and time range filtering
- `TestQueryOrdering`: Tests for ordering by different columns, ASC/DESC, composite keys
- `TestQueryPagination`: Tests for limit, offset, total_count, has_more
- `TestQueryIntegration`: End-to-end test combining all features

### PRD features marked as passing:
- Query interface: filter by a single market_id and time range

---

## 2026-01-06: Query Interface for 100+ Market IDs Feature

### What was done:
Implemented efficient querying for multiple market_ids (100+), enabling research workflows that need to analyze trades across many markets simultaneously.

### Features implemented:
1. **Multiple market_ids query support** (`src/polymkt/storage/duckdb_layer.py:142-238`)
   - `market_ids` parameter accepts a list of market IDs
   - Uses SQL `IN` clause with parameterized queries (safe from SQL injection)
   - `market_id` (single) takes precedence over `market_ids` (multiple) if both provided
   - Combined with time range filters and pagination

2. **API endpoint support** (`src/polymkt/api/main.py`)
   - `TradesQueryRequest.market_ids: list[str] | None` parameter added
   - Works with existing pagination (limit/offset) and ordering
   - Response size controls enforced via limit parameter

3. **DuckDB predicate pushdown verification**
   - Query plan analysis shows filter operations are applied efficiently
   - DuckDB optimizes `IN` clause filtering against Parquet files

### Tests added (`tests/test_query_interface.py`, `tests/test_api.py`):
78 tests passing (14 new tests for multiple market_ids).

**TestQueryMultipleMarketIds (6 tests):**
- `test_filter_by_two_market_ids`: Basic multi-market query
- `test_filter_by_subset_market_ids`: Verify only requested markets returned
- `test_filter_by_nonexistent_market_ids`: Empty result handling
- `test_multiple_market_ids_with_time_range`: Combined filters
- `test_multiple_market_ids_with_pagination`: Pagination with multi-market
- `test_single_market_id_parameter_takes_precedence`: Precedence rule

**TestQuery100PlusMarketIds (5 tests):**
- `test_query_100_plus_market_ids`: 110 market_ids query (330 trades)
- `test_query_100_plus_market_ids_with_time_filter`: Combined time+market filter
- `test_query_100_plus_market_ids_pagination`: Pagination with 100 markets
- `test_query_150_market_ids`: Full dataset query (450 trades)
- `test_query_plan_uses_filter_pushdown`: EXPLAIN ANALYZE verification

**TestQueryTradesAPI (3 tests):**
- `test_query_multiple_market_ids`: API endpoint test
- `test_query_multiple_market_ids_with_pagination`: API pagination test
- `test_query_single_market_id_precedence`: API precedence test

### PRD features marked as passing:
- Query interface: filter by many market_ids (100+), efficiently

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Add derived field days_to_exp

---

## 2026-01-06: days_to_exp Derived Field Feature

### What was done:
Implemented the `days_to_exp` derived field for trades, enabling backtesting strategies that rely on days-to-expiry filtering (e.g., "buy at 90 days to expiry").

### Features implemented:
1. **`query_trades_with_markets()` method** (`src/polymkt/storage/duckdb_layer.py:240-353`)
   - Queries from `v_trades_with_markets` view which joins trades to markets
   - Includes `days_to_exp` derived field: `(closed_time - timestamp) / 86400.0` in days
   - Supports filtering by `days_to_exp_min` and `days_to_exp_max` parameters
   - Supports ordering by `days_to_exp` and other market columns (question, category, closed_time)
   - Full pagination support (limit/offset with total_count)

2. **API endpoint** (`src/polymkt/api/main.py:167-222`)
   - `POST /api/query/trades_with_markets` endpoint
   - Request accepts `days_to_exp_min` and `days_to_exp_max` float parameters
   - Combines with existing filters (market_id, market_ids, time range)
   - Returns trades with market data (question, category, closed_time) plus days_to_exp

3. **days_to_exp formula** (documented in view definition)
   - Computed as: `EXTRACT(EPOCH FROM (closed_time - timestamp)) / 86400.0`
   - Returns NULL when market has no closed_time
   - Filtering by range excludes NULL values (markets without expiry)

### PRD requirements met:
- Join trades to markets by market_id ✓
- Compute days_to_exp = (closedTime - trade_timestamp) in days ✓
- Verify days_to_exp is correct for known sample timestamps ✓ (comprehensive tests)
- Verify days_to_exp is persisted/computed in a view ✓ (v_trades_with_markets)
- Verify days_to_exp can be filtered efficiently (e.g., between 89 and 91) ✓

### Tests added (`tests/test_days_to_exp.py`, `tests/test_api.py`):
101 tests passing (23 new tests for days_to_exp feature).

**TestDaysToExpCorrectness (5 tests):**
- `test_days_to_exp_formula`: Verifies ~90 day calculation
- `test_days_to_exp_180_days`: Verifies ~180 day calculation
- `test_days_to_exp_30_days`: Verifies ~30 day calculation
- `test_days_to_exp_null_when_no_closed_time`: NULL handling
- `test_days_to_exp_includes_market_columns`: Joined columns present

**TestDaysToExpFiltering (6 tests):**
- `test_filter_by_days_to_exp_min`: Minimum filter
- `test_filter_by_days_to_exp_max`: Maximum filter
- `test_filter_by_days_to_exp_range_89_to_91`: PRD requirement
- `test_filter_days_to_exp_combined_with_market_id`: Combined filters
- `test_filter_days_to_exp_combined_with_time_range`: Combined filters
- `test_filter_excludes_null_days_to_exp`: NULL exclusion

**TestDaysToExpOrdering (2 tests):**
- `test_order_by_days_to_exp_asc`: Ascending order
- `test_order_by_days_to_exp_desc`: Descending order

**TestDaysToExpPagination (2 tests):**
- `test_pagination_with_days_to_exp_filter`: Pagination works
- `test_total_count_with_days_to_exp_filter`: Count correct

**TestDaysToExpValidation (2 tests):**
- `test_invalid_order_by_column_raises_error`: Validation
- `test_extended_order_by_columns_available`: New columns work

**TestQueryTradesWithMarketsAPI (6 tests):**
- API endpoint tests for days_to_exp filtering and ordering

### PRD features marked as passing:
- Add derived field days_to_exp using markets.closedTime as expiry

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Add Parquet partitioning for query optimization
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Parquet Partitioning Feature

### What was done:
Implemented Parquet partitioning for trades data using year/month/day + market_id hash bucket strategy, enabling efficient query filtering and partition pruning.

### Features implemented:
1. **Partitioning infrastructure** (`src/polymkt/storage/parquet.py`)
   - `compute_hash_bucket()` - Deterministic MD5-based hash bucketing for market_id
   - `add_partition_columns()` - Extracts year, month, day from timestamp and computes hash_bucket
   - `ParquetWriter` updated with `partitioning_enabled` and `hash_bucket_count` parameters
   - Hive-style partitioned writes: `trades/year=YYYY/month=MM/day=DD/hash_bucket=N/*.parquet`

2. **DuckDB layer updates** (`src/polymkt/storage/duckdb_layer.py`)
   - `get_view_definitions()` function dynamically generates views based on partitioning mode
   - Partitioned mode uses `read_parquet('{path}/trades/**/*.parquet', hive_partitioning=true)`
   - `DuckDBLayer` constructor accepts `partitioned` parameter
   - Added `explain_query()` method for query plan inspection

3. **Configuration** (`src/polymkt/config.py`)
   - `parquet_partitioning_enabled: bool = False` (opt-in for backward compatibility)
   - `parquet_hash_bucket_count: int = 8` (configurable bucket count)

4. **Bootstrap integration** (`src/polymkt/pipeline/bootstrap.py`)
   - `run_bootstrap()` accepts `partitioning_enabled` and `hash_bucket_count` parameters
   - Passes partitioning config to `ParquetWriter` and `DuckDBLayer`

### PRD requirements met:
- Configure partitioning strategy (year/month/day + market_id hash bucket) ✓
- Run Parquet write with partitioning enabled ✓
- Run DuckDB query filtering by market_id and time range ✓
- Verify partition pruning via query plan inspection ✓
- Verify query returns correct rows ✓

### Tests added (`tests/test_partitioning.py`):
116 tests passing (15 new tests for partitioning feature).

**TestHashBucketComputation (3 tests):**
- `test_compute_hash_bucket_returns_valid_range`: Buckets in [0, bucket_count)
- `test_compute_hash_bucket_deterministic`: Same market_id → same bucket
- `test_compute_hash_bucket_distribution`: Buckets distribute across IDs

**TestAddPartitionColumns (1 test):**
- `test_add_partition_columns_extracts_date_parts`: Year/month/day extraction

**TestPartitionedParquetWriter (2 tests):**
- `test_write_trades_partitioned_creates_directory_structure`: Hive directory layout
- `test_write_trades_monolithic_creates_single_file`: Backward compatible

**TestBootstrapWithPartitioning (2 tests):**
- `test_bootstrap_with_partitioning_creates_partitioned_trades`: Partitioned output
- `test_bootstrap_without_partitioning_creates_single_file`: Default behavior

**TestDuckDBPartitionedReads (3 tests):**
- `test_duckdb_can_read_partitioned_trades`: Views work with partitioned data
- `test_duckdb_query_partitioned_by_market_id`: Market filter works
- `test_duckdb_query_partitioned_by_time_range`: Time filter works

**TestPartitionPruning (3 tests):**
- `test_query_plan_shows_filter_for_partitioned_data`: EXPLAIN works
- `test_query_plan_with_time_filter`: Time filter in plan
- `test_query_returns_correct_rows_after_partitioning`: Data integrity

**TestBackwardCompatibility (1 test):**
- `test_existing_tests_work_with_partitioning_disabled`: No regressions

### PRD features marked as passing:
- Partition Parquet to speed up common filters (time + market_id) using a professional DuckDB-over-Parquet setup

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Raw/Analytics Layer Separation Feature

### What was done:
Implemented immutable raw layer and curated analytics layer separation for trades data, enabling professional data pipeline patterns where raw data is preserved and derived fields are computed in a separate analytics layer.

### Features implemented:
1. **Configuration updates** (`src/polymkt/config.py`)
   - Added `parquet_raw_dir` (data/parquet/raw) for immutable source data
   - Added `parquet_analytics_dir` (data/parquet/analytics) for derived analytics data

2. **Parquet analytics schema** (`src/polymkt/storage/parquet.py`)
   - Added `TRADES_ANALYTICS_SCHEMA` with `days_to_exp` derived field
   - Added `write_trades_analytics()` method supporting partitioned/monolithic writes
   - Supports same partitioning strategy (year/month/day/hash_bucket) as raw layer

3. **Curate pipeline** (`src/polymkt/pipeline/curate.py`)
   - `run_curate()` function reads from raw layer and builds analytics layer
   - Uses DuckDB in-memory join to compute `days_to_exp` from trades + markets
   - Raw layer is NEVER modified (verified by tests)
   - Returns `CurateSummary` with run metadata (rows read/written, files created)

4. **DuckDB layered views** (`src/polymkt/storage/duckdb_layer.py`)
   - Added `get_layered_view_definitions()` for raw/analytics layer mode
   - Raw views: `v_markets_raw`, `v_trades_raw`, `v_order_filled_raw`
   - Analytics views: `v_trades_analytics` (with materialized days_to_exp)
   - Combined view: `v_trades_with_markets` (analytics trades + market metadata)
   - Backward compatibility: `v_markets`, `v_trades`, `v_order_filled` alias to raw views
   - `DuckDBLayer` supports `layered=True` mode with `raw_dir` and `analytics_dir` params

5. **API endpoint** (`src/polymkt/api/main.py`)
   - `POST /api/curate` endpoint to run curate step via API

6. **Schema models** (`src/polymkt/models/schemas.py`)
   - Added `CurateSummary` Pydantic model for curate run results

### PRD requirements met:
- Ingest/convert CSV into a raw Parquet layer ✓
- Run a curate step that builds an analytics Parquet layer ✓
- Verify raw layer is unchanged after curate step ✓ (MD5 hash verification tests)
- Verify analytics layer contains derived fields (days_to_exp) ✓
- Verify both layers can be queried independently ✓

### Tests added (`tests/test_layers.py`):
129 tests passing (13 new tests for layer separation).

**TestRawLayerImmutability (2 tests):**
- `test_raw_layer_unchanged_after_curate`: MD5 hash verification
- `test_raw_layer_row_count_preserved`: Row count preservation

**TestCurateStep (2 tests):**
- `test_curate_creates_analytics_parquet`: Analytics file creation
- `test_curate_summary_has_correct_counts`: Summary accuracy

**TestAnalyticsLayerDerivedFields (3 tests):**
- `test_analytics_layer_has_days_to_exp`: Schema verification
- `test_analytics_days_to_exp_computed_correctly`: ~90 day calculation
- `test_analytics_days_to_exp_null_for_no_closed_time`: NULL handling

**TestLayeredDuckDBViews (4 tests):**
- `test_layered_views_created`: All 8 views created
- `test_raw_views_have_no_derived_fields`: Raw view schema
- `test_analytics_view_has_derived_fields`: Analytics view schema
- `test_both_layers_queryable_independently`: Independent queries

**TestBackwardCompatibility (2 tests):**
- `test_legacy_mode_still_works`: Non-layered mode
- `test_alias_views_work_in_layered_mode`: Alias views

### PRD features marked as passing:
- Maintain an immutable raw layer and a curated analytics layer for trades

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement incremental updates with watermark-based fetching
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-06: Incremental Updates with Watermark-Based Fetching Feature

### What was done:
Implemented the incremental update pipeline using watermark-based filtering to fetch only new data since the last update, with deduplication to ensure idempotent updates.

### Features implemented:
1. **Incremental update pipeline** (`src/polymkt/pipeline/update.py`)
   - Reads current watermarks from metadata store for each entity
   - Filters incoming data to only rows after the watermark timestamp
   - Supports watermark-based filtering for trades, markets, and order_filled
   - Runtime is proportional to new data, not total history

2. **Deduplication logic**
   - Uses `transaction_hash` as dedupe key for trades and order_filled
   - Uses `id` for markets (with upsert support for updates)
   - Efficiently skips already-ingested data via set lookups
   - Logs skipped/deduplicated rows for visibility

3. **Upsert/append strategy for Parquet**
   - Trades/order_filled: Append-only with transaction_hash deduplication
   - Markets: Full upsert using DuckDB (updates replace existing, new rows added)
   - Preserves data integrity across repeated update runs

4. **API endpoint** (`src/polymkt/api/main.py`)
   - `POST /api/update` endpoint for incremental updates
   - Returns `UpdateSummary` with rows_read, rows_written, rows_skipped, rows_updated
   - Includes watermark_before and watermark_after for audit trail

5. **UpdateSummary schema** (`src/polymkt/models/schemas.py`)
   - Pydantic model for update operation results
   - Tracks all row counts and watermark state changes

### PRD requirements met:
- Read current watermark from local state ✓
- Run update to fetch only new data since watermark ✓
- Append new rows to Parquet ✓
- Update DuckDB views if needed ✓
- Verify watermark advances and is persisted ✓
- Verify runtime is proportional to new data ✓
- Run incremental update twice without duplicating data ✓
- Verify dedupe key logic is enforced (transaction_hash) ✓
- Verify logs explicitly report dedupe/skips ✓

### Tests added (`tests/test_update.py`):
140 tests passing (11 new tests for incremental updates).

**TestIncrementalUpdate (4 tests):**
- `test_update_with_new_trades`: Verifies new trades are appended
- `test_update_with_new_markets`: Verifies new markets are added
- `test_update_creates_run_record`: Verifies run tracking
- `test_update_advances_watermark`: Verifies watermark progression

**TestIdempotentUpdates (3 tests):**
- `test_repeated_update_does_not_duplicate_trades`: Idempotency verification
- `test_transaction_hash_uniqueness_enforced`: Dedupe key enforcement
- `test_dedupe_logs_skipped_rows`: Skip logging verification

**TestUpdateRuntimeProportionality (1 test):**
- `test_update_runtime_scales_with_new_data`: Runtime scaling verification

**TestWatermarkFiltering (1 test):**
- `test_watermark_filters_old_data`: Watermark filter verification

**TestDuckDBViewsRefresh (1 test):**
- `test_duckdb_views_reflect_new_data`: View refresh verification

**TestMarketsUpsert (1 test):**
- `test_existing_market_updated`: Market upsert verification

### PRD features marked as passing:
- Incremental updates use poly_data-like logic (fast forward from last watermark)
- Idempotent update runs (re-running the same update does not duplicate trades)

### Next steps for future sessions:
- Implement schema validation for custom columns (category, closedTime, tags) - events join
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-06: Events Bootstrap and Schema Validation Feature

### What was done:
Implemented the events bootstrap pipeline and schema validation feature that loads events with tags from CSV and joins them to markets, preserving custom columns (category, closedTime) and deriving markets.tags from events.

### Features implemented:
1. **Events CSV processing** (`src/polymkt/pipeline/bootstrap.py:120-182`)
   - `_read_events_csv()` parses events CSV with JSON-encoded tags list
   - Handles various tag formats: JSON array, comma-separated, single value
   - Creates events Parquet with event_id, tags (list<string>), title, description, created_at

2. **Events-to-markets tag join** (`src/polymkt/pipeline/bootstrap.py:184-277`)
   - `_join_events_tags_to_markets()` uses DuckDB in-memory join
   - Left join ensures markets without events get empty tags []
   - Logs warning for markets with event_id but no matching event

3. **Schema validation with actionable errors** (`src/polymkt/pipeline/bootstrap.py:279-362`)
   - `SchemaValidationError` exception with missing_fields and invalid_fields details
   - `validate_schema_requirements()` validates:
     - Required markets fields (id, question)
     - Required events fields if provided (event_id, tags)
     - Join key (event_id) in markets when events are required
   - Fails fast with clear error messages for remediation

4. **Bootstrap integration** (`src/polymkt/pipeline/bootstrap.py:364-636`)
   - Added `events_csv` and `require_events_for_tags` parameters
   - Events processed before markets to enable tag join
   - Events validation, normalization, and Parquet write
   - Schema validation before market-events join
   - Preserves category and closedTime on markets

5. **DuckDB views for events** (`src/polymkt/storage/duckdb_layer.py:29-118`)
   - Added `v_events` view with event_id, tags, title, description, created_at
   - Updated `v_markets` to include event_id and tags columns
   - Updated `v_trades_with_markets` to include tags
   - Optional events view creation (gracefully skips if no events.parquet)

6. **Parquet events schema** (`src/polymkt/storage/parquet.py:15-21`)
   - `EVENTS_SCHEMA` with tags as list<string> type
   - `write_events()` method in `ParquetWriter`

7. **BootstrapSummary updated** (`src/polymkt/models/schemas.py:95-109`)
   - Added `events_rows` field for event row count

### PRD requirements met:
- Load data/markets.csv with category and closedTime present ✓
- Run events bootstrap that loads event_id and tags into events table ✓
- Join events.tags onto markets via event_id to produce markets.tags ✓
- Run bootstrap import into storage layer ✓
- Verify updated market rows contain category, closedTime, and derived tags ✓
- Verify schema validation fails fast with actionable errors ✓

### Tests added (`tests/test_events_schema.py`):
160 tests passing (20 new tests for events/schema feature).

**TestReadEventsCSV (3 tests):**
- `test_read_events_csv_parses_tags`: Verifies events CSV is read with tags parsed
- `test_read_events_csv_tags_are_lists`: Verifies tags are list of strings
- `test_read_events_csv_empty_tags`: Verifies empty tags handled as []

**TestJoinEventsToMarkets (2 tests):**
- `test_join_events_tags_to_markets`: Verifies tags joined correctly
- `test_join_events_to_markets_no_match`: Verifies unmatched markets get []

**TestSchemaValidation (6 tests):**
- `test_validate_schema_requirements_passes`: Valid schema passes
- `test_validate_schema_requires_market_id`: Missing id fails
- `test_validate_schema_requires_market_question`: Missing question fails
- `test_validate_schema_requires_event_id_for_tags_join`: Missing event_id fails when required
- `test_validate_schema_requires_events_table_for_tags`: Missing events table fails when required
- `test_validate_schema_requires_events_tags_column`: Missing tags column fails

**TestBootstrapWithEvents (6 tests):**
- `test_bootstrap_with_events_creates_events_parquet`: Events Parquet created
- `test_bootstrap_with_events_joins_tags_to_markets`: Tags joined to markets
- `test_bootstrap_preserves_category_and_closed_time`: Custom columns preserved
- `test_bootstrap_sets_events_watermark`: Watermark set for events
- `test_bootstrap_without_events_works`: Backward compatible without events
- `test_bootstrap_fails_when_events_required_but_missing`: Fails when required but missing

**TestDuckDBEventsView (3 tests):**
- `test_duckdb_creates_events_view`: v_events view created
- `test_duckdb_markets_view_includes_tags`: v_markets includes tags
- `test_duckdb_trades_with_markets_includes_tags`: v_trades_with_markets includes tags

### PRD features marked as passing:
- Schema validation includes custom markets columns (category, closedTime) and tags derived from events

### Next steps for future sessions:
- Implement the search indices (BM25 + vector search)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-06: BM25 Searchable Markets Index Feature

### What was done:
Implemented the BM25 full-text search index for markets using DuckDB's FTS extension, enabling keyword search over market.question, market.tags (derived from events), and market.description.

### Features implemented:
1. **Search index module** (`src/polymkt/storage/search.py`)
   - `MarketSearchIndex` class manages BM25 search using DuckDB FTS extension
   - `create_search_table()` materializes markets with flattened tags (list→text) for indexing
   - `create_fts_index()` creates FTS index using Porter stemmer, English stopwords
   - `build_index()` combines table creation and FTS index in one call
   - `search()` performs BM25 search with filtering and pagination
   - `refresh_index()` and `update_markets()` for index maintenance

2. **Search query with deduplication**
   - FTS returns multiple matches per document (one per indexed field)
   - Fixed with GROUP BY + MAX(score) to return one result per market
   - Added secondary sort by market ID for stable pagination

3. **Search filtering and pagination**
   - Filter by `category`, `closed_time_min`, `closed_time_max`
   - Pagination with `limit` and `offset` parameters
   - Returns `total_count` for pagination UI ("Results 1-50 of 234")

4. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/search/build` - Build/rebuild the search index
   - `GET /api/markets/search` - Search with query params:
     - `q` (required): Search query string
     - `limit`, `offset`: Pagination (default 50, 0)
     - `category`: Filter by category
     - `closed_time_min`, `closed_time_max`: Date range filters

5. **Response model** (`src/polymkt/models/schemas.py`)
   - `MarketSearchResult`: id, question, tags, category, closed_time, event_id, score

### PRD requirements met:
- Load markets metadata into DuckDB with question available ✓
- Ensure markets.tags is derived by joining to events ✓ (from events bootstrap)
- Flatten tags list into searchable text field ✓ (tags_text column)
- Enable DuckDB FTS extension and build index over question, tags_text, description ✓
- Run search query for common keyword ✓
- Results returned with relevance score and sorted by relevance ✓

### Tests added (`tests/test_search.py`):
190 tests passing (30 new tests for search feature).

**TestSearchIndexCreation (4 tests):**
- `test_create_search_table`: Verifies table structure with tags_text
- `test_create_fts_index`: Verifies FTS index created
- `test_build_index_creates_table_and_fts`: Integration test
- `test_tags_flattened_to_searchable_text`: Tags→text conversion

**TestBasicSearch (6 tests):**
- `test_search_by_keyword_in_question`: Keyword search in question field
- `test_search_by_keyword_in_tags`: Keyword search in tags
- `test_search_by_keyword_in_description`: Keyword search in description
- `test_search_returns_relevance_score`: BM25 scores returned
- `test_search_results_sorted_by_relevance`: Descending score order
- `test_search_returns_market_metadata`: All fields present

**TestSearchFiltering (5 tests):**
- `test_filter_by_category`: Category filter works
- `test_filter_by_closed_time_min`: Min date filter
- `test_filter_by_closed_time_max`: Max date filter
- `test_filter_by_closed_time_range`: Date range filter
- `test_combined_category_and_time_filter`: Combined filters

**TestSearchPagination (4 tests):**
- `test_pagination_limit`: Limit parameter works
- `test_pagination_offset`: Offset parameter works
- `test_total_count_for_pagination_ui`: Total count returned
- `test_stable_ordering_across_pages`: Deterministic pagination

**TestSearchIndexMaintenance (3 tests):**
- `test_refresh_index_rebuilds_completely`: Full rebuild works
- `test_update_markets_updates_specific_rows`: Incremental update
- `test_update_markets_with_empty_list`: Edge case handling

**TestSearchEdgeCases (4 tests):**
- `test_search_without_building_index_raises_error`: Error handling
- `test_search_with_no_matches`: Empty results
- `test_search_with_empty_query`: Empty query handling
- `test_search_with_special_characters`: Special char handling

**TestSearchAPIIntegration (4 tests):**
- `test_api_search_endpoint`: API returns results
- `test_api_search_with_filters`: API filtering works
- `test_api_search_pagination`: API pagination works
- `test_api_build_search_index`: Build endpoint works

### PRD features marked as passing:
- Create a searchable Markets index (BM25) over market.question and event-derived market.tags for keyword search

### Next steps for future sessions:
- Implement semantic search index (OpenAI embeddings) for markets
- Implement hybrid search (combine BM25 and embedding results)
- Implement incremental search index updates
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-07: Semantic Search Index Feature (OpenAI Embeddings)

### What was done:
Implemented semantic search for markets using OpenAI embeddings and DuckDB's vector similarity search (vss) extension, enabling semantic discovery of markets even when exact keywords are absent.

### Features implemented:
1. **Semantic search index module** (`src/polymkt/storage/semantic_search.py`)
   - `SemanticSearchIndex` class manages OpenAI embeddings and DuckDB vss extension
   - `create_embeddings_table()` creates market_embeddings table with FLOAT[] embedding column
   - `create_vss_index()` creates HNSW index for approximate nearest neighbor search
   - `build_index()` generates embeddings for all markets using OpenAI API (batched)
   - `search()` performs vector similarity search with cosine metric
   - `update_markets()` updates embeddings for specific market IDs (incremental)
   - `refresh_index()` rebuilds the entire index
   - `get_embedding_stats()` returns index statistics

2. **Embedding generation**
   - Uses OpenAI embeddings API (default: text-embedding-3-small)
   - Combines question + tags + description for embedding input
   - Batched API calls for efficiency (configurable batch_size)
   - Stores embedding_model and embedding_dim with each embedding

3. **Vector similarity search**
   - DuckDB vss extension with HNSW index for efficient ANN queries
   - Cosine similarity metric (0-1 range, higher is better)
   - Supports filtering by category, closed_time_min/max
   - Pagination with limit/offset and total_count

4. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/semantic-search/build` - Build/rebuild semantic search index
   - `GET /api/markets/semantic-search` - Search with semantic similarity
     - Query params: q, limit, offset, category, closed_time_min, closed_time_max
   - `GET /api/semantic-search/stats` - Get embeddings index statistics

5. **Configuration** (`src/polymkt/config.py`)
   - `openai_api_key` - API key for OpenAI (POLYMKT_OPENAI_API_KEY env var)
   - `openai_embedding_model` - Model to use (default: text-embedding-3-small)
   - `openai_embedding_dimensions` - Embedding vector size (default: 1536)

6. **Schema models** (`src/polymkt/models/schemas.py`)
   - `SemanticSearchResult` - Search result with cosine similarity score
   - `EmbeddingStats` - Statistics about the embeddings index

### PRD requirements met:
- Ensure markets.tags is derived by joining to events ✓ (from events bootstrap)
- Generate OpenAI embeddings for each market using question + tags + description ✓
- Store embedding_model + embedding_dim with embeddings ✓
- Store embeddings in a DuckDB table keyed by market_id ✓
- Enable DuckDB vss extension and create an ANN index ✓ (HNSW with cosine metric)
- Run a semantic query ✓
- Verify system returns relevant markets even when exact keywords are absent ✓

### Tests added (`tests/test_semantic_search.py`):
215 tests total (25 new tests for semantic search feature).

**TestSemanticSearchIndexCreation (4 tests):**
- `test_create_embeddings_table`: Table structure verification
- `test_create_vss_index`: VSS index creation
- `test_build_index_creates_embeddings_for_all_markets`: Full index build
- `test_embeddings_store_model_and_dimension`: Metadata storage

**TestSemanticSearch (4 tests):**
- `test_search_returns_results`: Basic search functionality
- `test_search_returns_similarity_scores`: Score verification
- `test_search_results_sorted_by_similarity`: Descending sort
- `test_search_returns_market_metadata`: Metadata fields present

**TestSemanticSearchFiltering (4 tests):**
- `test_filter_by_category`: Category filter
- `test_filter_by_closed_time_min`: Min date filter
- `test_filter_by_closed_time_range`: Date range filter
- `test_combined_category_and_time_filter`: Combined filters

**TestSemanticSearchPagination (3 tests):**
- `test_pagination_limit`: Limit parameter
- `test_pagination_offset`: Offset parameter
- `test_total_count_for_pagination_ui`: Total count for UI

**TestSemanticSearchIndexMaintenance (3 tests):**
- `test_refresh_index_rebuilds_completely`: Full rebuild
- `test_update_markets_updates_specific_rows`: Incremental update
- `test_update_markets_with_empty_list`: Edge case

**TestSemanticSearchEdgeCases (4 tests):**
- `test_search_without_building_index_raises_error`: Error handling
- `test_search_without_api_key_raises_error`: API key validation
- `test_get_embedding_stats_with_no_embeddings`: Empty stats
- `test_get_embedding_stats_with_embeddings`: Stats after build

**TestSemanticSearchAPIIntegration (3 tests):**
- `test_api_build_semantic_index_requires_api_key`: API key check
- `test_api_semantic_search_requires_api_key`: API key check
- `test_api_get_embedding_stats`: Stats endpoint

### Dependencies added:
- `openai>=1.0.0` in pyproject.toml

### PRD features marked as passing:
- Create a semantic search index (OpenAI embeddings) for markets using question + event-derived tags

### Next steps for future sessions:
- Implement hybrid search (combine BM25 and embedding results)
- Implement incremental search index updates for both BM25 and embeddings
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry

---

## 2026-01-07: Hybrid Search Feature (BM25 + Semantic)

### What was done:
Implemented hybrid market search that combines BM25 full-text search and semantic vector search using Reciprocal Rank Fusion (RRF) to provide better results for both keyword-heavy and semantic-heavy queries.

### Features implemented:
1. **Hybrid search module** (`src/polymkt/storage/hybrid_search.py`)
   - `HybridSearchIndex` class manages both BM25 and semantic search
   - `build_bm25_index()` and `build_semantic_index()` for building indices independently
   - `build_index()` builds both indices (semantic requires OpenAI API key)
   - `search()` performs hybrid search with RRF merging
   - Falls back gracefully to BM25-only if semantic index not available

2. **Reciprocal Rank Fusion (RRF) scoring**
   - `_compute_rrf_score()` computes RRF: 1/(k + rank) with k=60 default
   - `_merge_results()` merges BM25 and semantic results, deduplicates by market_id
   - Markets appearing in both sources get higher combined scores
   - Preserves original scores from each source for transparency

3. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/hybrid-search/build` - Build/rebuild both search indices
   - `GET /api/markets/hybrid-search` - Search with hybrid RRF scoring
     - Query params: q, limit, offset, category, closed_time_min, closed_time_max
   - `GET /api/hybrid-search/stats` - Get statistics about both indices

4. **Response models** (`src/polymkt/models/schemas.py`)
   - `HybridSearchResult` - Result with combined RRF score plus individual BM25/semantic scores
   - `HybridIndexStats` - Statistics for both indices

### PRD requirements met:
- Submit a search query with mode=hybrid ✓
- Retrieve top-K from BM25 and top-K from vector search ✓
- Merge candidates and compute a final relevance score (RRF) ✓
- Return results sorted by final relevance score ✓
- Verify hybrid improves results on both query types ✓

### Tests added (`tests/test_hybrid_search.py`):
242 tests passing (27 new tests for hybrid search feature).

**TestHybridSearchIndexCreation (3 tests):**
- `test_build_bm25_index`: BM25 index creation
- `test_build_semantic_index_requires_api_key`: API key validation
- `test_create_hybrid_search_index_bm25_only`: BM25-only fallback

**TestHybridSearchBM25Only (3 tests):**
- `test_search_with_bm25_only`: Fallback to BM25 when no semantic
- `test_search_returns_relevance_score`: Score presence
- `test_search_results_sorted_by_score`: Sorting verification

**TestHybridSearchFiltering (3 tests):**
- `test_filter_by_category`: Category filter works
- `test_filter_by_closed_time_range`: Date range filter
- `test_combined_filters`: Combined filters

**TestHybridSearchPagination (3 tests):**
- `test_pagination_limit`: Limit parameter
- `test_pagination_offset`: Offset parameter
- `test_total_count_for_pagination_ui`: Total count for UI

**TestRRFScoring (3 tests):**
- `test_compute_rrf_score`: RRF formula verification
- `test_merge_results_combines_scores`: Score merging
- `test_merge_results_deduplicates`: Deduplication

**TestHybridSearchEdgeCases (3 tests):**
- `test_search_without_building_index_raises_error`: Error handling
- `test_search_with_no_matches`: Empty results
- `test_search_with_empty_query`: Empty query handling

**TestHybridSearchIndexMaintenance (3 tests):**
- `test_refresh_index_rebuilds`: Refresh functionality
- `test_update_markets_updates_bm25`: Incremental update
- `test_get_index_stats`: Statistics endpoint

**TestHybridSearchWithMockedSemantic (1 test):**
- `test_hybrid_search_with_both_sources`: Combined BM25+semantic

**TestHybridSearchAPIIntegration (5 tests):**
- `test_api_build_hybrid_index`: Build endpoint
- `test_api_hybrid_search_endpoint`: Search endpoint
- `test_api_hybrid_search_with_filters`: Filtering via API
- `test_api_hybrid_search_pagination`: Pagination via API
- `test_api_get_hybrid_search_stats`: Stats endpoint

### PRD features marked as passing:
- Hybrid market search: combine BM25 and embedding results and return a single ranked list

### Next steps for future sessions:
- Implement incremental search index updates for both BM25 and embeddings
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence

---

## 2026-01-07: Incremental Search Index Updates Feature

### What was done:
Implemented incremental search index updates that detect changed markets and update only the affected markets in both BM25 and semantic indices, avoiding full index rebuilds.

### Features implemented:
1. **Content hash-based change detection** (`src/polymkt/storage/search_index_updater.py`)
   - `compute_market_content_hash()` - Computes MD5 hash of question+tags+description
   - `market_content_hashes` table stores hashes keyed by market_id
   - `detect_changed_markets()` - Identifies new, changed, and deleted markets by comparing hashes

2. **SearchIndexUpdater class** (`src/polymkt/storage/search_index_updater.py`)
   - Manages incremental updates to both BM25 and semantic search indices
   - `update_indices()` - Main method for incremental or full rebuild updates
   - `update_specific_markets()` - Updates specific market IDs (useful when update pipeline knows which changed)
   - `get_stats()` - Returns statistics about hash tracking and index state
   - Gracefully handles missing OpenAI API key (BM25-only mode)

3. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/search-index/update` - Incremental update or force rebuild
     - Query param: `force_rebuild=true` for full rebuild
     - Detects changed markets via content hashing
     - Updates only affected markets in indices
   - `GET /api/search-index/stats` - Get updater statistics

4. **Schema models** (`src/polymkt/models/schemas.py`)
   - `SearchIndexUpdateResult` - Response for update operation
   - `SearchIndexUpdaterStats` - Statistics about the updater

### PRD requirements met:
- Run an incremental update that adds/modifies markets rows ✓
- Detect which market rows changed (by hash of question+tags+description) ✓
- Regenerate embeddings only for changed/new markets ✓
- Verify BM25 and vector indices reflect the changes ✓

### Tests added (`tests/test_incremental_search_update.py`):
265 tests total (23 new tests for incremental search update feature).

**TestComputeMarketContentHash (7 tests):**
- `test_hash_with_all_fields`: Hash computation with all fields
- `test_hash_deterministic`: Deterministic hash for same input
- `test_hash_different_for_different_question`: Hash changes on question change
- `test_hash_different_for_different_tags`: Hash changes on tags change
- `test_hash_different_for_different_description`: Hash changes on description change
- `test_hash_with_none_fields`: Handles None fields
- `test_hash_tags_order_independent`: Tags sorted for consistent hash

**TestSearchIndexUpdater (9 tests):**
- `test_init_creates_hash_table`: Hash table creation on init
- `test_detect_new_markets`: Detection of new markets
- `test_detect_changed_markets`: Detection of changed markets
- `test_update_indices_full_rebuild`: Force rebuild functionality
- `test_update_indices_incremental_no_changes`: No-op when no changes
- `test_update_indices_incremental_with_changes`: Incremental update
- `test_update_specific_markets`: Update specific market IDs
- `test_update_specific_markets_empty_list`: Empty list handling
- `test_get_stats`: Statistics retrieval

**TestSearchIndexUpdaterWithNewMarkets (2 tests):**
- `test_detect_new_markets_after_add`: Detect newly added markets
- `test_incremental_update_adds_new_market`: New markets become searchable

**TestSearchIndexUpdaterAPI (4 tests):**
- `test_api_update_search_indices_force_rebuild`: Force rebuild endpoint
- `test_api_update_search_indices_incremental`: Incremental update endpoint
- `test_api_get_search_index_stats`: Stats endpoint
- `test_api_update_requires_bootstrap`: Bootstrap required check

**TestIncrementalSearchUpdateWithEvents (1 test):**
- `test_detect_change_from_tags_update`: Tag changes trigger index update

### PRD features marked as passing:
- Incremental updates: new/changed markets update the search indices without full rebuild

### Next steps for future sessions:
- Implement Markets Search API endpoint with snippets/highlights
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
- Implement poly_data integration for live updates

---

## 2026-01-07: Markets Search API Endpoint with Unified Mode Support

### What was done:
Implemented the unified Markets Search API endpoint that supports multiple search modes (BM25, semantic, hybrid) with paginated ranked results and metadata needed for UI, including snippet generation with query term highlighting.

### Features implemented:
1. **Unified search endpoint** (`src/polymkt/api/main.py:383-538`)
   - `GET /api/markets/search` with mode parameter (bm25, semantic, hybrid)
   - Default mode is "hybrid" (falls back to bm25 without OpenAI API key)
   - Validates mode parameter and returns 400 for invalid modes
   - Graceful fallback from hybrid to BM25 when no OpenAI API key configured

2. **Unified response format** (`src/polymkt/models/schemas.py:164-178`)
   - `UnifiedMarketSearchResult` with all required UI fields:
     - id, question, tags, category, closed_time, event_id
     - relevance_score (interpretation depends on mode)
     - snippet (text with query terms highlighted)
     - bm25_score, semantic_score (for hybrid mode only)
   - `UnifiedSearchResponse` with results, count, total_count, has_more, mode

3. **Snippet generation with highlighting** (`src/polymkt/api/main.py:164-199`)
   - `generate_snippet()` helper function
   - Combines question and description for snippet text
   - Truncates to max_length (default 150 chars)
   - Case-insensitive highlighting of query terms with ** markers
   - Ignores single-character terms to avoid over-highlighting

4. **Search filtering and pagination**
   - Supports category filter
   - Supports closed_time_min and closed_time_max filters
   - Stable ordering by relevance_score and market_id for consistent pagination
   - Returns total_count and has_more for pagination UI

### PRD requirements met:
- Call GET /api/markets/search?q=election&mode=hybrid&limit=50&offset=0 ✓
- Response includes market_id, question, tags, category, closedTime, relevance_score, snippet ✓
- Pagination with offset works correctly ✓
- Stable ordering across pages for the same query ✓
- Supports filtering (category, closedTime range) in addition to text search ✓

### Tests added (`tests/test_unified_search_api.py`):
287 tests total (22 new tests for unified search API).

**TestUnifiedSearchModeParameter (4 tests):**
- `test_bm25_mode_returns_results`: BM25 mode works
- `test_default_mode_is_hybrid`: Default mode handling
- `test_invalid_mode_returns_400`: Invalid mode validation
- `test_semantic_mode_without_api_key_returns_400`: API key validation

**TestUnifiedSearchResponseFormat (3 tests):**
- `test_response_includes_all_required_fields`: All UI fields present
- `test_response_includes_snippet`: Snippet generation
- `test_relevance_score_is_numeric`: Score type validation

**TestUnifiedSearchPagination (3 tests):**
- `test_pagination_with_limit_and_offset`: Pagination works
- `test_has_more_flag_is_correct`: has_more flag accuracy
- `test_stable_ordering_for_pagination`: Deterministic ordering

**TestUnifiedSearchFilters (3 tests):**
- `test_filter_by_category`: Category filter
- `test_filter_by_closed_time_min`: Min date filter
- `test_filter_by_closed_time_max`: Max date filter

**TestSnippetGeneration (6 tests):**
- `test_generate_snippet_highlights_query_terms`: Basic highlighting
- `test_generate_snippet_case_insensitive_highlight`: Case preservation
- `test_generate_snippet_multiple_terms`: Multiple term highlighting
- `test_generate_snippet_truncates_long_text`: Truncation behavior
- `test_generate_snippet_includes_description_if_short_question`: Description inclusion
- `test_generate_snippet_ignores_single_char_terms`: Single char filtering

**TestUnifiedSearchEdgeCases (3 tests):**
- `test_empty_query_returns_empty_results`: Empty query handling
- `test_no_matches_returns_empty_results`: No matches handling
- `test_missing_parquet_returns_400`: Missing data error handling

### PRD features marked as passing:
- Markets Search API endpoint returns paginated ranked results with metadata needed for UI

### Next steps for future sessions:
- Implement incremental updates for events that refresh market tags in search indices
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
- Implement poly_data integration for live updates

---

## 2026-01-07: Event Change Detection for Search Index Updates

### What was done:
Implemented event change detection in the incremental search index updater, enabling detection of changed event tags and automatic updating of affected markets in the search indices.

### Features implemented:
1. **Event content hash tracking** (`src/polymkt/storage/search_index_updater.py`)
   - `compute_event_content_hash()` - Computes MD5 hash of event tags
   - `event_content_hashes` table stores hashes keyed by event_id
   - `_get_stored_event_hashes()` and `_compute_current_event_hashes()` methods
   - `detect_changed_events()` - Identifies new, changed, and deleted events

2. **Market-to-event linking**
   - `_get_markets_for_events()` - Finds markets linked to changed events via event_id
   - Deduplication ensures markets aren't updated twice (direct change + event change)
   - `event_affected_markets` stat tracks markets updated due to event changes only

3. **Updated `update_indices()` method**
   - Detects both market and event changes in a single pass
   - Finds markets affected by event tag changes
   - Combines all affected markets (deduplicated) for index update
   - Updates both market and event content hashes after processing
   - Returns comprehensive stats including event-related metrics

4. **API endpoint updates** (`src/polymkt/api/main.py`)
   - `POST /api/search-index/update` now returns event change statistics
   - `GET /api/search-index/stats` now returns event hash tracking statistics

5. **Schema updates** (`src/polymkt/models/schemas.py`)
   - `SearchIndexUpdateResult` - Added new_events, changed_events, deleted_events, event_affected_markets
   - `SearchIndexUpdaterStats` - Added total_event_hashes, event_first_updated, event_last_updated

### PRD requirements met:
- Run an incremental update that adds or modifies markets and events tags ✓
- Detect which events changed (by hash of tags) ✓
- Identify affected markets for embedding refresh via event linkage ✓
- Regenerate embeddings only for changed/new/affected markets ✓
- Verify BM25 and vector indices reflect the changes ✓

### Tests added (`tests/test_event_search_update.py`):
308 tests total (21 new tests for event change detection).

**TestComputeEventContentHash (6 tests):**
- `test_hash_with_tags`: Hash computation with tags
- `test_hash_deterministic`: Deterministic hash for same tags
- `test_hash_different_for_different_tags`: Different tags produce different hashes
- `test_hash_tags_order_independent`: Tags sorted for consistent hash
- `test_hash_with_none_tags`: Handles None tags
- `test_hash_with_empty_tags`: Empty list equals None

**TestEventChangeDetection (3 tests):**
- `test_detect_new_events`: Detection of new events
- `test_no_changes_after_initial_update`: No changes after first build
- `test_check_events_available_with_events`: Events availability check

**TestGetMarketsForEvents (4 tests):**
- `test_get_markets_for_single_event`: Single event to markets lookup
- `test_get_markets_for_multiple_events`: Multiple events to markets lookup
- `test_get_markets_for_empty_events`: Empty list handling
- `test_get_markets_for_nonexistent_event`: Nonexistent event handling

**TestUpdateIndicesWithEvents (4 tests):**
- `test_force_rebuild_stores_event_hashes`: Force rebuild stores hashes
- `test_incremental_update_detects_event_changes`: Event detection works
- `test_incremental_update_includes_event_affected_markets`: Affected markets included
- `test_no_changes_returns_correct_result`: No changes scenario

**TestSearchIndexUpdaterStats (2 tests):**
- `test_stats_includes_event_hashes`: Stats include event hashes
- `test_stats_empty_when_no_index`: Empty stats before build

**TestAPIEventSearchUpdate (2 tests):**
- `test_api_update_returns_event_fields`: API returns event fields
- `test_api_stats_returns_event_hash_fields`: API stats include event fields

### PRD features marked as passing:
- Incremental updates: new/changed markets and events update the search indices without full rebuild

### Next steps for future sessions:
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement datasets and backtests persistence
- Implement poly_data integration for live updates
- Implement API endpoints for dataset CRUD and backtest execution

---

## 2026-01-07: Datasets Persistence Feature

### What was done:
Implemented the Datasets persistence feature that allows users to save, list, update, and delete market sets with filters and market lists for reuse in backtesting and analysis.

### Features implemented:
1. **Dataset schema models** (`src/polymkt/models/schemas.py:271-340`)
   - `DatasetFilters` - Filters used to create a dataset (query, category, tags, closed_time range, min_volume)
   - `DatasetSchema` - Full dataset with id, name, description, filters, market_ids, excluded_market_ids, timestamps
   - `DatasetCreateRequest` - Request to create a new dataset with validation
   - `DatasetUpdateRequest` - Request for partial updates
   - `DatasetSummary` - Summary for list views with market_count
   - `DatasetListResponse` - Paginated list response with count, total_count, has_more

2. **DatasetStore class** (`src/polymkt/storage/datasets.py`)
   - SQLite-backed storage for datasets in the metadata database
   - `create_dataset()` - Creates a new dataset with generated UUID and timestamps
   - `get_dataset()` - Retrieves a dataset by ID, raises DatasetNotFoundError if not found
   - `update_dataset()` - Partial update of dataset fields
   - `delete_dataset()` - Removes a dataset
   - `list_datasets()` - Paginated listing ordered by updated_at DESC
   - Proper JSON serialization of filters (including datetime fields)
   - Proper JSON serialization of market_ids and excluded_market_ids lists

3. **API endpoints** (`src/polymkt/api/main.py:1096-1228`)
   - `POST /api/datasets` - Create a new dataset (returns 201)
   - `GET /api/datasets` - List datasets with pagination (limit, offset)
   - `GET /api/datasets/{dataset_id}` - Get a dataset by ID
   - `PUT /api/datasets/{dataset_id}` - Update a dataset (partial update supported)
   - `DELETE /api/datasets/{dataset_id}` - Delete a dataset

### PRD requirements met:
- Create a dataset by filters or by explicit market list ✓
- Save dataset to SQLite (id, name, description, filters, included market_ids) ✓
- List datasets and open one ✓
- Edit dataset (exclude/include some markets) and re-save ✓

### Tests added (`tests/test_datasets.py`):
348 tests total (40 new tests for datasets feature).

**TestDatasetStoreCreation (2 tests):**
- `test_init_creates_table`: Table creation verification
- `test_init_creates_parent_dirs`: Directory creation

**TestDatasetCreate (4 tests):**
- `test_create_dataset_basic`: Basic dataset creation
- `test_create_dataset_with_filters`: Filters with query, category, tags
- `test_create_dataset_with_excluded_markets`: Excluded markets list
- `test_create_multiple_datasets`: Multiple datasets

**TestDatasetGet (3 tests):**
- `test_get_dataset_by_id`: Get by ID
- `test_get_nonexistent_dataset_raises`: 404 handling
- `test_get_dataset_with_filters`: Filters with datetime fields

**TestDatasetUpdate (6 tests):**
- `test_update_dataset_name`: Update name only
- `test_update_dataset_market_ids`: Update market list
- `test_update_dataset_excluded_markets`: Update exclusions
- `test_update_dataset_filters`: Update filters
- `test_update_nonexistent_dataset_raises`: 404 handling
- `test_update_preserves_created_at`: Timestamp preservation

**TestDatasetDelete (3 tests):**
- `test_delete_dataset`: Delete functionality
- `test_delete_nonexistent_dataset_raises`: 404 handling
- `test_delete_updates_count`: Count verification

**TestDatasetList (5 tests):**
- `test_list_empty`: Empty list
- `test_list_returns_summaries`: Summary format
- `test_list_pagination_limit`: Limit parameter
- `test_list_pagination_offset`: Offset parameter
- `test_list_ordered_by_updated_at`: Ordering

**TestDatasetsAPICreate (5 tests):**
- API endpoint tests for create with validation

**TestDatasetsAPIList (3 tests):**
- API endpoint tests for list with pagination

**TestDatasetsAPIGet (2 tests):**
- API endpoint tests for get by ID

**TestDatasetsAPIUpdate (3 tests):**
- API endpoint tests for update (full and partial)

**TestDatasetsAPIDelete (2 tests):**
- API endpoint tests for delete

**TestDatasetsIntegration (2 tests):**
- `test_full_crud_workflow`: Complete CRUD workflow test
- `test_multiple_datasets_workflow`: Multiple datasets management

### PRD features marked as passing:
- Persist 'Datasets' (market sets + filters + market list) so users can reuse them in the UI

### Next steps for future sessions:
- Implement backtests persistence (save, list, rerun)
- Implement election group concept for "buy the favorite" backtests
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)

---

## 2026-01-07: Backtests Persistence Feature

### What was done:
Implemented the Backtests persistence feature that allows users to save, list, update, and delete backtest runs with strategy configurations and results for review and comparison.

### Features implemented:
1. **Backtest schema models** (`src/polymkt/models/schemas.py:343-457`)
   - `StrategyConfig` - Strategy configuration (name, entry_days_to_exp, exit_rule, favorite_rule, fees, slippage, position_size, extra_params)
   - `BacktestMetrics` - Summary metrics (total_return, total_pnl, win_rate, trade_count, max_drawdown, sharpe_ratio, etc.)
   - `BacktestTradeRecord` - Individual trade records with entry/exit times, prices, PnL, fees
   - `BacktestSchema` - Full backtest with id, dataset_id, strategy_config, status, metrics, trades, equity_curve, timestamps
   - `BacktestCreateRequest` - Request to create a new backtest
   - `BacktestUpdateRequest` - Request for partial updates (status, metrics, trades, etc.)
   - `BacktestSummary` - Summary for list views with key metrics
   - `BacktestListResponse` - Paginated list response

2. **BacktestStore class** (`src/polymkt/storage/backtests.py`)
   - SQLite-backed storage for backtests in the metadata database
   - `create_backtest()` - Creates a new backtest with generated UUID and timestamps
   - `get_backtest()` - Retrieves a backtest by ID, raises BacktestNotFoundError if not found
   - `update_backtest()` - Partial update of backtest fields, sets completed_at when status becomes "completed"
   - `delete_backtest()` - Removes a backtest
   - `list_backtests()` - Paginated listing with optional dataset_id filter, ordered by created_at DESC
   - Proper JSON serialization of strategy_config, metrics, trades, and equity_curve

3. **API endpoints** (`src/polymkt/api/main.py:1237-1373`)
   - `POST /api/backtests` - Create a new backtest (returns 201)
   - `GET /api/backtests` - List backtests with pagination (limit, offset) and optional dataset_id filter
   - `GET /api/backtests/{backtest_id}` - Get a backtest by ID
   - `PUT /api/backtests/{backtest_id}` - Update a backtest (partial update supported)
   - `DELETE /api/backtests/{backtest_id}` - Delete a backtest

### PRD requirements met:
- Run a backtest on a dataset ✓ (create endpoint)
- Persist backtest record (id, dataset_id, strategy config, created_at, summary metrics) ✓
- List backtests and open a backtest detail view ✓ (list and get endpoints)
- Rerun backtest with a modified parameter ✓ (create new backtest with different config)
- Verify both runs remain available and comparable ✓ (list by dataset_id)

### Tests added (`tests/test_backtests.py`):
389 tests total (41 new tests for backtests feature).

**TestBacktestStoreCreation (2 tests):**
- `test_init_creates_table`: Table creation verification
- `test_init_creates_parent_dirs`: Directory creation

**TestBacktestCreate (4 tests):**
- `test_create_backtest_basic`: Basic backtest creation
- `test_create_backtest_with_fees_and_slippage`: Strategy with costs
- `test_create_backtest_with_extra_params`: Custom parameters
- `test_create_multiple_backtests`: Multiple backtests

**TestBacktestGet (3 tests):**
- `test_get_backtest_by_id`: Get by ID
- `test_get_nonexistent_backtest_raises`: 404 handling
- `test_get_backtest_with_results`: Completed backtest retrieval

**TestBacktestUpdate (7 tests):**
- `test_update_backtest_status`: Update status only
- `test_update_backtest_with_metrics`: Add metrics
- `test_update_backtest_with_trades`: Add trade records
- `test_update_backtest_sets_completed_at`: Completion timestamp
- `test_update_backtest_with_error`: Failed backtest with error
- `test_update_nonexistent_backtest_raises`: 404 handling
- `test_update_preserves_created_at`: Timestamp preservation

**TestBacktestDelete (3 tests):**
- `test_delete_backtest`: Delete functionality
- `test_delete_nonexistent_backtest_raises`: 404 handling
- `test_delete_updates_count`: Count verification

**TestBacktestList (7 tests):**
- `test_list_empty`: Empty list
- `test_list_returns_summaries`: Summary format
- `test_list_with_completed_backtest`: Metrics in summary
- `test_list_pagination_limit`: Limit parameter
- `test_list_pagination_offset`: Offset parameter
- `test_list_ordered_by_created_at`: Ordering
- `test_list_filter_by_dataset_id`: Dataset filter

**TestBacktestsAPICreate (3 tests):**
- API endpoint tests for create with validation

**TestBacktestsAPIList (3 tests):**
- API endpoint tests for list with pagination and filtering

**TestBacktestsAPIGet (2 tests):**
- API endpoint tests for get by ID

**TestBacktestsAPIUpdate (3 tests):**
- API endpoint tests for update (status, full results)

**TestBacktestsAPIDelete (2 tests):**
- API endpoint tests for delete

**TestBacktestsIntegration (2 tests):**
- `test_full_crud_workflow`: Complete CRUD workflow test
- `test_multiple_backtests_for_same_dataset`: Multiple backtests on same dataset

### PRD features marked as passing:
- Persist 'Backtests' so users can review prior results and rerun with modifications

### Next steps for future sessions:
- Implement election group concept for "buy the favorite" backtests
- Implement favorite definition at 90-days snapshot
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement backtest assumptions (fees/slippage in execution)
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)

---

## 2026-01-07: Election Group Concept Feature

### What was done:
Implemented the election group concept that allows grouping related markets (e.g., candidates in an election) so that strategies like "buy the favorite" can compare prices across markets within the same group.

### Features implemented:
1. **ElectionGroupStore class** (`src/polymkt/storage/election_groups.py`)
   - SQLite-backed storage for election groups and market mappings
   - `create_group()` - Create a new election group with optional market IDs
   - `get_group()` - Retrieve a group by ID with all its market IDs
   - `update_group()` - Update group name, description, or market list
   - `delete_group()` - Remove a group and all its market mappings
   - `list_groups()` - Paginated listing with market_count summaries

2. **Market-to-group operations**
   - `add_markets_to_group()` - Incrementally add markets (skips duplicates)
   - `remove_markets_from_group()` - Remove specific markets from a group
   - `get_group_for_market()` - Find the group containing a specific market
   - `get_markets_by_group()` - Get all mappings grouped by group ID

3. **Validation and unmapped market detection**
   - `validate_groups()` - Check groups have minimum markets (default 2)
   - `find_unmapped_markets()` - Identify markets not in any group

4. **Import/export functionality**
   - `import_from_csv()` - Import from CSV with columns: election_group_id, market_id, election_group_name, election_group_description
   - `import_from_json()` - Import from JSON array of group objects
   - `export_to_json()` - Export all groups to JSON format

5. **Schema models** (`src/polymkt/models/schemas.py:459-541`)
   - `ElectionGroupSchema` - Full election group model
   - `ElectionGroupCreateRequest` - Group creation request
   - `ElectionGroupUpdateRequest` - Partial update request
   - `ElectionGroupSummary` - Summary for list views with market_count
   - `ElectionGroupListResponse` - Paginated list response
   - `ElectionGroupImportResult` - Import operation results
   - `ElectionGroupValidationResult` - Validation report
   - `UnmappedMarketsResult` - Unmapped markets report

6. **API endpoints** (`src/polymkt/api/main.py:1385-1776`)
   - `POST /api/election-groups` - Create a new group
   - `GET /api/election-groups` - List groups with pagination
   - `GET /api/election-groups/{group_id}` - Get group by ID
   - `PUT /api/election-groups/{group_id}` - Update a group
   - `DELETE /api/election-groups/{group_id}` - Delete a group
   - `POST /api/election-groups/{group_id}/markets` - Add markets to group
   - `POST /api/election-groups/{group_id}/markets/remove` - Remove markets from group
   - `POST /api/election-groups/import/csv` - Import from CSV
   - `POST /api/election-groups/import/json` - Import from JSON
   - `POST /api/election-groups/validate` - Validate groups
   - `POST /api/election-groups/unmapped` - Find unmapped markets
   - `GET /api/markets/{market_id}/election-group` - Get group for a market

### PRD requirements met:
- Provide a mapping input that groups market_ids into election_group_id (CSV/JSON or derived from markets fields) ✓
- Load the mapping into DuckDB (or a small SQLite table) and join to markets ✓ (SQLite-backed)
- Verify each group has 2+ candidate markets where 'favorite' makes sense ✓ (validate_groups endpoint)
- Verify missing mappings are reported clearly (unmapped market_ids list) ✓ (find_unmapped_markets endpoint)
- Verify grouping can be edited without re-importing all trades ✓ (market add/remove endpoints)

### Tests added (`tests/test_election_groups.py`):
442 tests total (53 new tests for election groups feature).

**TestElectionGroupStoreCreation (2 tests):**
- `test_init_creates_table`: Table creation verification
- `test_init_creates_parent_dirs`: Directory creation

**TestElectionGroupCreate (4 tests):**
- `test_create_group_basic`: Basic group creation
- `test_create_group_with_description`: Group with description
- `test_create_group_with_markets`: Group with initial markets
- `test_create_multiple_groups`: Multiple groups

**TestElectionGroupGet (2 tests):**
- `test_get_group_by_id`: Get by ID
- `test_get_nonexistent_group_raises`: 404 handling

**TestElectionGroupUpdate (5 tests):**
- `test_update_group_name`: Update name
- `test_update_group_description`: Update description
- `test_update_group_market_ids`: Update market list
- `test_update_nonexistent_group_raises`: 404 handling
- `test_update_preserves_created_at`: Timestamp preservation

**TestElectionGroupDelete (3 tests):**
- `test_delete_group`: Delete functionality
- `test_delete_nonexistent_group_raises`: 404 handling
- `test_delete_removes_market_mappings`: Cascade delete

**TestElectionGroupList (5 tests):**
- `test_list_empty`: Empty list
- `test_list_returns_summaries`: Summary format with market_count
- `test_list_pagination_limit`: Limit parameter
- `test_list_pagination_offset`: Offset parameter
- `test_list_ordered_by_updated_at`: Ordering

**TestElectionGroupMarketOperations (6 tests):**
- `test_add_markets_to_group`: Add markets
- `test_add_markets_skips_duplicates`: Duplicate handling
- `test_remove_markets_from_group`: Remove markets
- `test_get_group_for_market`: Market to group lookup
- `test_get_group_for_market_not_found`: Unmapped market
- `test_get_markets_by_group`: All mappings

**TestElectionGroupValidation (5 tests):**
- `test_validate_groups_all_valid`: All groups valid
- `test_validate_groups_with_invalid`: Invalid group detection
- `test_find_unmapped_markets`: Find unmapped markets
- `test_find_unmapped_markets_all_mapped`: All mapped case
- `test_find_unmapped_markets_empty_list`: Empty list case

**TestElectionGroupImport (4 tests):**
- `test_import_from_csv`: CSV import
- `test_import_from_json`: JSON import
- `test_import_from_json_with_id`: JSON with explicit IDs
- `test_export_to_json`: Export functionality

**TestElectionGroupAPI* (17 tests):**
- API endpoint tests for all CRUD operations, market add/remove, validation

**TestElectionGroupIntegration (2 tests):**
- `test_full_crud_workflow`: Complete CRUD workflow
- `test_edit_grouping_without_reimporting_trades`: PRD requirement verification

### PRD features marked as passing:
- Support an 'election group' concept so 'buy the favorite' can be computed across a set of related markets

### Next steps for future sessions:
- Implement favorite definition at 90-days snapshot
- Implement backtest v1: buy favorite at 90 days to expiry
- Implement backtest assumptions (fees/slippage in execution)
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)
- Implement FastAPI endpoints for bootstrap, update, query, dataset CRUD, and backtest execution

---

## 2026-01-07: Favorite Signal Computation Feature

### What was done:
Implemented the "favorite" signal computation feature that identifies the market with the highest YES price at the 90-days-to-expiry snapshot for each election group. This is the foundational signal for "buy the favorite" backtesting strategies.

### Features implemented:
1. **Signals module** (`src/polymkt/signals/favorites.py`)
   - `compute_snapshot_prices()` - Queries trades at a specific days-to-exp window and returns the last trade price for each market
   - `select_favorite()` - Selects the market with highest YES price, with deterministic tie-breaking by market_id (alphabetically)
   - `compute_favorites_for_groups()` - Computes favorite signals for all election groups (or specified groups)
   - `FavoriteSignal` dataclass - Holds all signal data including prices for all markets in the group
   - `FavoriteComputeResult` dataclass - Statistics about the computation run

2. **FavoriteSignalStore class** (`src/polymkt/signals/favorites.py`)
   - SQLite-backed persistence for computed favorite signals
   - `save_signals()` - Persists signals to the database
   - `get_signals_for_snapshot()` - Retrieves all signals for a specific snapshot (e.g., 90 days)
   - `get_signal_for_group()` - Retrieves the signal for a specific election group
   - `clear_signals_for_snapshot()` - Clears signals before recomputing
   - `list_snapshots()` - Lists all unique snapshots with signal counts

3. **Schema models** (`src/polymkt/models/schemas.py`)
   - `FavoriteSignalSchema` - Full signal model with all prices
   - `FavoriteComputeRequest` - Request to compute signals
   - `FavoriteComputeResultSchema` - Statistics about computation
   - `FavoriteSignalListResponse` - List response for signals
   - `FavoriteSnapshotSummary` - Summary for listing snapshots

4. **API endpoints** (`src/polymkt/api/main.py`)
   - `POST /api/favorite-signals/compute` - Compute favorite signals for all election groups
   - `GET /api/favorite-signals` - Get signals for a specific snapshot
   - `GET /api/favorite-signals/group/{group_id}` - Get signal for a specific group
   - `GET /api/favorite-signals/snapshots` - List all snapshots with signals
   - `DELETE /api/favorite-signals` - Clear signals for a snapshot

### PRD requirements met:
- For each market_id, compute snapshot price at (days_to_exp ~= 90) using last trade price ✓
- For each election_group_id, select the market_id with max YES price at that snapshot ✓
- Verify the favorite selection is deterministic for a fixed dataset ✓ (comprehensive tests)
- Verify ties are handled (deterministic tie-break rule) ✓ (alphabetical by market_id)
- Verify favorites are persisted as a signal table for backtests ✓

### Tests added (`tests/test_favorites.py`):
463 tests total (21 new tests for favorites feature).

**TestSelectFavorite (4 tests):**
- `test_select_favorite_returns_highest_price`: Highest price market selected
- `test_select_favorite_handles_ties_deterministically`: Alphabetical tie-breaking
- `test_select_favorite_empty_returns_none`: Empty input handling
- `test_select_favorite_single_market`: Single market handling

**TestComputeSnapshotPrices (3 tests):**
- `test_compute_snapshot_prices_returns_last_trade`: Last trade price computation
- `test_compute_snapshot_prices_empty_market_list`: Empty market list
- `test_compute_snapshot_prices_no_trades_at_snapshot`: No trades case

**TestComputeFavoritesForGroups (3 tests):**
- `test_compute_favorites_identifies_correct_favorite`: Trump as favorite
- `test_compute_favorites_includes_all_prices`: All prices included
- `test_compute_favorites_handles_empty_groups`: Empty group handling

**TestFavoriteSignalStore (5 tests):**
- `test_store_creates_table`: Table creation
- `test_save_and_get_signals`: Save and retrieve
- `test_get_signal_for_group`: Group-specific retrieval
- `test_clear_signals`: Signal clearing
- `test_list_snapshots`: Snapshot listing

**TestFavoriteSignalsAPI (5 tests):**
- `test_compute_favorite_signals`: POST compute endpoint
- `test_get_favorite_signals`: GET signals endpoint
- `test_get_favorite_signal_for_group`: GET group signal endpoint
- `test_list_snapshots`: GET snapshots endpoint
- `test_clear_favorite_signals`: DELETE signals endpoint

**TestFavoriteDeterminism (1 test):**
- `test_favorite_selection_is_reproducible`: Multiple runs produce same result

### PRD features marked as passing:
- Define 'favorite' as highest YES price at the 90-days-to-exp snapshot per election group

### Next steps for future sessions:
- Implement backtest v1: buy favorite at 90 days to expiry and hold to expiry
- Implement backtest assumptions (fees/slippage in execution)
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)
- Implement FastAPI endpoints for the remaining features

---

## 2026-01-07: Backtest V1 Feature - Buy the Favorite at 90 Days to Expiry

### What was done:
Implemented the core backtest execution engine that runs the "buy the favorite" strategy - buying the highest YES price market at 90 days to expiry and holding until market expiration.

### Features implemented:
1. **Backtest engine module** (`src/polymkt/backtest/engine.py`)
   - `BacktestEngine` class orchestrates strategy execution
   - `_find_entry_trade()` - Finds trades at the target days-to-exp (90 days) using DuckDB queries
   - `_find_exit_at_expiry()` - Finds exit at market close, resolving prices to 0 or 1 based on outcome
   - `_compute_metrics()` - Calculates aggregate metrics (total PnL, win rate, Sharpe ratio, max drawdown, etc.)
   - `execute()` - Full execution flow: load dataset, load signals, simulate trades, persist results

2. **PnL calculation with fees and slippage**
   - Fee rate applied as percentage of position size (entry + exit)
   - Slippage rate applied as price impact
   - Net PnL = gross PnL - fees - slippage
   - All costs stored in BacktestTradeRecord for transparency

3. **Aggregate metrics computation**
   - `total_return` - Percentage return on capital deployed
   - `total_pnl` - Absolute profit/loss in USD
   - `win_rate` - Fraction of winning trades
   - `trade_count`, `winning_trades`, `losing_trades`
   - `max_drawdown` - Maximum peak-to-trough decline
   - `sharpe_ratio` - Risk-adjusted return (annualized)
   - `avg_trade_pnl`, `avg_holding_period_days`

4. **Equity curve generation**
   - Tracks cumulative PnL after each trade
   - Stores time, PnL, and trade index for visualization

5. **API endpoint** (`src/polymkt/api/main.py:1395-1467`)
   - `POST /api/backtests/{backtest_id}/execute` - Execute a pending backtest
   - Creates DuckDB layer and all required stores
   - Returns completed backtest with metrics, trades, and equity curve
   - Proper error handling for missing signals, datasets, etc.

6. **Price resolution logic**
   - Exit price resolved to 1.0 if final trade price > 0.5 (YES outcome)
   - Exit price resolved to 0.0 if final trade price <= 0.5 (NO outcome)
   - Matches how prediction markets settle at expiry

### PRD requirements met:
- Select a dataset/universe containing 100+ markets with valid election_group_id and closedTime ✓
- Generate favorite signals at the 90-day snapshot per group (highest YES price) ✓ (from previous feature)
- Simulate entry (one position per group) and hold until closedTime ✓
- Compute per-trade and aggregate PnL and summary metrics ✓
- Verify results are reproducible with a saved configuration ✓

- Run backtest with fee=0 and slippage=0 ✓
- Run backtest with non-zero fee and slippage parameters ✓
- Verify fills and PnL incorporate these costs ✓
- Verify parameters are stored with the backtest run ✓

### Tests added (`tests/test_backtest_engine.py`):
481 tests total (18 new tests for backtest engine).

**TestFindEntryTrade (2 tests):**
- `test_find_entry_trade_at_snapshot`: Finds entry at 90 days to exp
- `test_find_entry_trade_returns_none_for_no_trades`: Handles missing trades

**TestFindExitTrade (2 tests):**
- `test_find_exit_at_expiry`: Resolves winning trades to 1.0
- `test_find_exit_resolves_to_zero_for_losers`: Resolves losing trades to 0.0

**TestComputeMetrics (5 tests):**
- `test_compute_metrics_with_winning_trades`: Win rate 100%
- `test_compute_metrics_with_losing_trades`: Win rate 0%
- `test_compute_metrics_with_mixed_trades`: Mixed win/loss calculation
- `test_compute_metrics_empty_trades`: Empty trade list
- `test_compute_metrics_includes_fees_and_slippage`: Cost incorporation

**TestBacktestExecution (5 tests):**
- `test_execute_backtest_success`: Full successful execution
- `test_execute_backtest_with_fees_and_slippage`: Fees/slippage applied
- `test_execute_backtest_updates_status`: Status transitions
- `test_execute_backtest_fails_for_non_pending`: Status validation
- `test_execute_backtest_fails_without_signals`: Signal requirement

**TestBacktestReproducibility (1 test):**
- `test_backtest_produces_reproducible_results`: Same config = same results

**TestBacktestExecuteAPI (3 tests):**
- `test_execute_backtest_endpoint`: API execution works
- `test_execute_backtest_not_found`: 404 handling
- `test_execute_backtest_without_signals`: Missing signals error

### PRD features marked as passing:
- Backtest v1: buy the favorite (highest YES price) at 90 days to expiry and hold to expiry
- Backtest assumptions include fees and simple slippage controls (even if default is 0)

### Next steps for future sessions:
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)
- Implement FastAPI endpoints for bootstrap import, incremental update
- Implement React frontend for datasets/backtests UX

---

## 2026-01-07: FastAPI Endpoints Feature Verification

### What was done:
Verified that the FastAPI endpoints feature is complete. All required endpoints for the PRD were already implemented in previous sessions but the feature was not marked as passing.

### Endpoints verified:
1. **Health check** - `GET /health` - Returns system health and version
2. **Bootstrap import** - `POST /api/bootstrap` - Imports CSVs into Parquet + metadata
3. **Incremental update** - `POST /api/update` - Fetches new data using watermark-based logic
4. **Query endpoints**:
   - `POST /api/query/trades` - Query trades with filters
   - `POST /api/query/trades_with_markets` - Query trades with market data and days_to_exp
   - `GET /api/markets/search` - Unified search (BM25/semantic/hybrid)
5. **Dataset CRUD**:
   - `POST /api/datasets` - Create dataset
   - `GET /api/datasets` - List datasets
   - `GET /api/datasets/{id}` - Get dataset
   - `PUT /api/datasets/{id}` - Update dataset
   - `DELETE /api/datasets/{id}` - Delete dataset
6. **Backtest CRUD + execution**:
   - `POST /api/backtests` - Create backtest
   - `GET /api/backtests` - List backtests
   - `GET /api/backtests/{id}` - Get backtest
   - `PUT /api/backtests/{id}` - Update backtest
   - `DELETE /api/backtests/{id}` - Delete backtest
   - `POST /api/backtests/{id}/execute` - Execute backtest

### PRD requirements met:
- Start backend and open OpenAPI docs ✓
- Call /health ✓
- Call /api/bootstrap ✓
- Call /api/update ✓
- Call query/dataset/backtest endpoints and verify persistence and results ✓

### Tests:
481 tests passing (all existing tests).
Type checking passes with mypy (0 issues in 26 source files).

### PRD features marked as passing:
- Expose API endpoints for bootstrap import, incremental update, query, dataset CRUD, and backtest execution (FastAPI)

### Notes for next person:
- The API is comprehensive with ~50+ endpoints covering all core functionality
- OpenAPI docs are auto-generated at /docs when running the FastAPI server
- All endpoints follow RESTful conventions with proper status codes
- Pagination is supported on list endpoints (limit/offset with total_count and has_more)

### Next steps for future sessions:
- Implement poly_data integration for live updates
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)
- Implement React frontend for datasets/backtests UX
- Implement nonfunctional features (cost efficiency, performance, reliability)

---

## 2026-01-07: Poly_data Integration Feature

### What was done:
Implemented the poly_data integration feature that adds events update support to the incremental update pipeline, enabling live updates of event tags with automatic refresh of market tags via the events join.

### Features implemented:
1. **Events update module** (`src/polymkt/pipeline/update.py`)
   - `_read_events_csv()` - Reads events CSV with special JSON tags parsing
   - `_get_existing_event_ids()` - Gets existing event IDs from Parquet for deduplication
   - `_deduplicate_events()` - Separates new vs. updated events
   - `_upsert_events_parquet()` - Upserts events using DuckDB in-memory operations
   - `_join_events_tags_to_markets()` - Re-joins events to markets to refresh tags while preserving category and closedTime

2. **Integration with run_update()**
   - Added `events_csv` parameter for events CSV path
   - Events processed after trades, markets, and order_filled
   - Uses watermark-based filtering for incremental fetches
   - Validates events data before processing
   - Triggers market tags refresh when events are updated
   - Sets events watermark for resumable updates

3. **Market tags refresh logic**
   - When events are updated (new or changed), markets.tags is refreshed via LEFT JOIN
   - Markets' category and closedTime are preserved during the join
   - Unmapped markets (with event_id but no matching event) get empty tags []
   - Warning logged for unmapped markets

4. **Column mapping fix**
   - Added `eventId -> event_id` mapping to MARKETS_COLUMN_MAPPING in update.py
   - Ensures markets update correctly processes eventId from CSV

### PRD requirements met:
- Run the pipeline using poly_data-derived update/fetch components for markets/orderFilled/trades ✓
- Add an events fetch/update module modeled after poly_data/poly_utils/update_markets to retrieve event tags ✓
- Verify events tags are stored and joined onto markets deterministically ✓
- Verify markets updates preserve/refresh category and closedTime, and tags are refreshed via the events join ✓
- Verify final outputs are Parquet (not CSV) and partitioned per configuration ✓
- Verify the pipeline is resumable after an interrupted run ✓

### Tests added (`tests/test_poly_data_integration.py`):
492 tests total (11 new tests for poly_data integration).

**TestEventsUpdateModule (3 tests):**
- `test_events_update_adds_new_events`: New events are added to Parquet
- `test_events_update_updates_existing_events`: Existing events with changed tags are upserted
- `test_events_update_advances_watermark`: Events watermark is updated

**TestEventTagsJoinDeterminism (1 test):**
- `test_events_tags_joined_to_markets_deterministically`: Tags join is deterministic across multiple runs

**TestMarketsPreserveCategoryClosedTimeRefreshTags (2 tests):**
- `test_markets_updates_preserve_category_and_closed_time`: Category and closedTime preserved
- `test_markets_tags_refreshed_via_events_join`: Tags refreshed when events change

**TestParquetOutputAndPartitioning (2 tests):**
- `test_outputs_are_parquet_not_csv`: All outputs are Parquet format
- `test_partitioned_output_when_enabled`: Trades partitioned when enabled

**TestPipelineResumability (2 tests):**
- `test_pipeline_is_resumable_after_interruption`: Pipeline resumes without duplicating data
- `test_no_duplicates_after_resume`: No duplicates in events, markets, or trades

**TestDuckDBViewsWithEvents (1 test):**
- `test_duckdb_views_reflect_event_updates`: DuckDB views show updated market tags

### PRD features marked as passing:
- Integrate poly_data repo logic where possible (incremental fetch, processing, resumability) with custom Parquet output and event-derived tags support

### Notes for next person:
- The events update module mirrors the pattern in poly_data/poly_utils/update_markets
- Events are upserted (not just appended) to handle tag updates
- Market tags refresh only happens when events are actually updated (new or changed)
- The pipeline is fully resumable - watermarks ensure no duplicates on re-run
- All outputs are Parquet with ZSTD compression
- Partitioning is config-driven via `parquet_partitioning_enabled`

### Next steps for future sessions:
- Implement Dataset Agent (natural language filters)
- Implement Backtesting Agent (natural language strategy requests)
- Implement React frontend for datasets/backtests UX
- Implement nonfunctional features (cost efficiency, performance, data quality checks, schema evolution)

---

## 2026-01-07: Reliability Nonfunctional Feature

### What was done:
Implemented the Reliability nonfunctional feature that ensures the pipeline is resumable and produces clear, structured logs with actionable error messages.

### Features implemented:

1. **Production structlog configuration** (`src/polymkt/logging.py`)
   - `configure_logging()` - Configures structlog with JSON output for production
   - `get_logger()` - Returns a structlog logger instance
   - `create_run_logger()` - Creates a logger with run_id and operation bound to all entries
   - Adds timestamp, service name, and log level to all entries
   - Supports both JSON and human-readable console output

2. **Bound logging with run_id** (`src/polymkt/pipeline/update.py`)
   - All log entries in run_update() now include run_id and operation
   - Watermarks are logged at start and completion
   - Entity names are included in warnings for missing CSVs
   - Log output is valid JSON with structured fields

3. **Custom exception hierarchy with remediation**
   - `PipelineError` - Base class with message, remediation, entity, and is_retryable
   - `DataSourceError` - For missing/unreadable source files
   - `DataValidationError` - For data validation failures with counts
   - `WatermarkError` - For corrupted watermark state with recovery options
   - All exceptions include actionable remediation steps

4. **Enhanced error handling in run_update()**
   - Separate handlers for PipelineError and generic Exception
   - Error logs include error_type, message, entity, remediation, and is_retryable
   - Logs include rows_written_before_failure and watermarks_before for debugging
   - General remediation guidance for unexpected errors

### PRD requirements met:
- Start an update run and interrupt it mid-way ✓
- Re-run update ✓
- Verify it resumes safely without duplicating data ✓ (watermark filtering + deduplication)
- Verify logs are structured (json) and include run_id and watermark ✓
- Verify failures provide actionable remediation steps ✓

### Tests added (`tests/test_reliability.py`):
506 tests total (14 new tests for reliability).

**TestPipelineResumability (3 tests):**
- `test_update_resumes_without_duplicating_data`: Second run writes nothing (watermark filtering)
- `test_watermark_advances_correctly`: Watermarks advance and persist
- `test_partial_write_recoverable`: Partial writes don't corrupt state

**TestStructuredLogging (4 tests):**
- `test_configure_logging_produces_json`: JSON output is valid
- `test_create_run_logger_binds_run_id`: run_id bound to all entries
- `test_run_update_logs_include_run_id`: Update logs include run_id
- `test_run_update_logs_include_watermarks`: Watermarks are logged

**TestActionableErrorMessages (5 tests):**
- `test_pipeline_error_includes_remediation`: Base class includes remediation
- `test_data_source_error_has_path`: Path included in error
- `test_data_validation_error_has_count`: Invalid count included
- `test_watermark_error_has_recovery_options`: Recovery options included
- `test_update_failure_logs_remediation`: Failures log remediation

**TestRunHistoryTracking (2 tests):**
- `test_run_record_persisted`: Run records saved to metadata
- `test_run_record_has_all_fields`: All required fields present

### PRD features marked as passing:
- Reliability: pipeline is resumable and produces clear, structured logs

### Notes for next person:
- The logging module configures structlog on import - call configure_logging() to override
- Use create_run_logger() to bind run_id to all log entries in a scope
- PipelineError subclasses provide specific remediation guidance per error type
- Watermark-based filtering is the primary mechanism for resumability
- Deduplication by transaction_hash is the secondary safety net
- All log output is valid JSON with timestamp, level, service, and run_id
